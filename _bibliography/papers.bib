@inproceedings{201410_verdoja_automatic,
  address     = {Paris, France},
  bibtex_show = {true},
  title       = {Automatic method for tumor segmentation from 3-points dynamic 
                 {PET} acquisitions},
  doi         = {10.1109/ICIP.2014.7025188},
  booktitle   = {2014 {IEEE} {Int.}\ {Conf.}\ on {Image} 
                 {Processing} ({ICIP})},
  author      = {Verdoja, Francesco and Grangetto, Marco and Bracco, Christian and 
                 Varetto, Teresio and Racca, Manuela and Stasi, Michele},
  month       = oct,
  year        = {2014},
  pages       = {937--941},
  award       = {Awarded for this work by *Microsoft Research Cambridge* at the *2014 International Computer Vision Summer School (ICVSS)*},
  award_name  = {Best Student Award},
  abstract    = {In this paper a novel technique to segment tumor voxels in 
                 dynamic positron emission tomography (PET) scans is proposed.
                 An innovative anomaly detection tool tailored for 3-points
                 dynamic PET scans is designed. The algorithm allows the
                 identification of tumoral cells in dynamic FDG-PET scans thanks
                 to their peculiar anaerobic metabolism experienced over time.
                 The proposed tool is preliminarily tested on a small dataset
                 showing promising performance as compared to the state of the
                 art in terms of both accuracy and classification errors.},
  pdf         = {https://iris.unito.it/retrieve/handle/2318/154166/148728/PID3258293_4aperto.pdf},
  abbr        = {ICIP},
  preview     = {icip_14.png}
}

@inproceedings{201509_fracastoro_superpixel-driven,
  address     = {Quebec City, Canada},
  bibtex_show = {true},
  title       = {Superpixel-driven {Graph} {Transform} for {Image} {Compression}},
  doi         = {10.1109/ICIP.2015.7351279},
  booktitle   = {2015 {IEEE} {Int.}\ {Conf.}\ on {Image} 
                 {Processing} ({ICIP})},
  author      = {Fracastoro, Giulia and Verdoja, Francesco and Grangetto, Marco 
                 and Magli, Enrico},
  month       = sep,
  year        = {2015},
  pages       = {2631--2635},
  award       = {Awarded at the 2015 IEEE Int.\ Conf.\ on Image Processing (ICIP)},
  award_name  = {Best 10\% Paper},
  funding     = {sisvel},
  abstract    = {Block-based compression tends to be inefficient when blocks 
                 contain arbitrary shaped discontinuities. Recently, graph-based
                 approaches have been proposed to address this issue, but the
                 cost of transmitting graph topology often overcome the gain of
                 such techniques. In this work we propose a new
                 Superpixel-driven Graph Transform (SDGT) that uses clusters of
                 superpixels, which have the ability to adhere nicely to edges
                 in the image, as coding blocks and computes inside these
                 homogeneously colored regions a graph transform which is
                 shape-adaptive. Doing so, only the borders of the regions and
                 the transform coefficients need to be transmitted, in place of
                 all the structure of the graph. The proposed method is finally
                 compared to DCT and the experimental results show how it is
                 able to outperform DCT both visually and in term of PSNR.},
  pdf         = {https://iris.unito.it/bitstream/2318/1557609/2/articleICIP15.pdf},
  abbr        = {ICIP},
  preview     = {icip_15.png}
}

@incollection{201509_verdoja_fast,
  series      = {Lecture {Notes} in {Computer} {Science}},
  bibtex_show = {true},
  title       = {Fast {Superpixel}-{Based} {Hierarchical} {Approach} to {Image} 
                 {Segmentation}},
  number      = {9279},
  doi         = {10.1007/978-3-319-23231-7_33},
  booktitle   = {Image {Analysis} and {Processing}---{ICIAP} 2015},
  publisher   = {Springer},
  author      = {Verdoja, Francesco and Grangetto, Marco},
  month       = sep,
  year        = {2015},
  pages       = {364--374},
  funding     = {sisvel},
  abstract    = {Image segmentation is one of the core task in image processing.
                 Traditionally such operation is performed starting from single
                 pixels requiring a significant amount of computations. Only
                 recently it has been shown that superpixels can be used to
                 improve segmentation performance. In this work we  propose a
                 novel superpixel-based hierarchical approach for image
                 segmentation that works by iteratively merging nodes of a
                 weighted undirected graph initialized with the superpixels
                 regions. Proper metrics to drive the regions merging are
                 proposed and experimentally validated using the standard
                 Berkeley Dataset. Our analysis shows that the proposed
                 algorithm runs faster than state of the art techniques while
                 providing accurate segmentation results both in terms of visual
                 and objective metrics.},
  pdf         = {https://link.springer.com/content/pdf/10.1007/978-3-319-23231-7_33.pdf},
  abbr        = {ICIAP},
  preview     = {iciap_15.jpg}
}

@incollection{201602_bracco_automatic,
  series      = {Physica Medica},
  bibtex_show = {true},
  title       = {Automatic {GTV} contouring applying anomaly detection algorithm on 
                 dynamic {FDG} {PET} images},
  number      = {1},
  volume      = {32},
  doi         = {10.1016/j.ejmp.2016.01.343},
  booktitle   = {Abstracts of the 9th National Congress of the Associazione Italiana di Fisica Medica},
  author      = {Bracco, Christian and Verdoja, Francesco and Grangetto, Marco and 
                 Di Dia, Amalia and Racca, Manuela and Varetto, Teresio and Stasi, 
                 Michele},
  month       = feb,
  year        = {2016},
  pages       = {99},
  abstract    = {Introduction: The aim of this work is to show the results of 
                 GTV automatic segmentation based on dynamic PET acquisition.
                 With respect to single voxel segmentation the temporal
                 information is used to improve quality of GTV delineation. The
                 segmentation algorithm proposed exploits the theoretic
                 assumption that FDG uptake over time in cancer cells is very
                 different from the one in normal tissues and therefore in this
                 study anomaly detection is used to look for tumor
                 peculiar-anomalous TACs. Material and Methods: For each patient
                 two list mode datasets of images were acquired. The first one
                 scan (basal) was acquired one hour after FDG injection and
                 reconstructed as static frame. The last one (delayed) was
                 acquired half one hour after the first scan and reconstructed
                 as dynamic scan. Two delayed scans were registered to the basal
                 scan. A modified version of the RX Detector was used. RX
                 Detector usually works in RGB, but in this study its use on
                 TACs has been explored passing the three grayscale images in
                 place of the three channels of RGB. The resulting single image,
                 which actually is a matrix of Mahalanobis distances, presents
                 values that are very high for voxels whose TAC has anomalous
                 temporal behavior. Finally, threshold segmentation is performed
                 on the distance matrix. On a dataset of 10 patients
                 segmentation techniques present in the literature working on
                 single PET scan have been implemented as well as segmentation
                 techniques based on RX Detector output. Results: Spatial
                 overlap index (SOI) was used as metric to evaluate the
                 segmentation accuracy. All of the segmentation algorithms
                 implemented on RXD output show better SOI (0.507 ± 0.158) than
                 algorithm based on SUV, i.e. Brambilla, SOI 0.278 ± 0.236. A
                 manual contour drawn by experienced Nuclear Physician was the
                 reference. Conclusion: Although a small dataset, the
                 segmentation of dynamic PET images based on RXD output seems to
                 be promising.},
  pdf         = {https://iris.unito.it/retrieve/e27ce428-8b03-2581-e053-d805fe0acbaa/copertina.pdf},
  abbr        = {AIFM},
  preview     = {aifm_16.jpg}
}

@inproceedings{201609_verdoja_global,
  address     = {Phoenix, AZ},
  bibtex_show = {true},
  title       = {Global and local anomaly detectors for tumor segmentation in 
                 dynamic {PET} acquisitions},
  doi         = {10.1109/ICIP.2016.7533137},
  booktitle   = {2016 {IEEE} {Int.}\ {Conf.}\ on {Image} 
                 {Processing} ({ICIP})},
  author      = {Verdoja, Francesco and Bonafè, Barbara and Cavagnino, Davide and 
                 Grangetto, Marco and Bracco, Christian and Varetto, Teresio and 
                 Racca, Manuela and Stasi, Michele},
  month       = sep,
  year        = {2016},
  pages       = {4131--4135},
  abstract    = {In this paper we explore the application of anomaly detection
                 techniques to tumor voxels segmentation. The developed
                 algorithms work on 3-points dynamic FDG-PET acquisitions and
                 leverage on the peculiar anaerobic metabolism that cancer cells
                 experience over time. A few different global or local anomaly
                 detectors are discussed, together with an investigation over
                 two different algorithms aiming to estimate normal tissues'
                 statistical distribution. Finally, all the proposed algorithms
                 are tested on a dataset composed of 9 patients proving that
                 anomaly detectors are able to outperform techniques in the
                 state of the art.},
  pdf         = {https://web.archive.org/web/20180718235804id_/https://iris.unito.it/retrieve/handle/2318/1610112/258226/ArticleTumors2016_IRIS.pdf},
  abbr        = {ICIP},
  preview     = {icip_16.png}
}

@patent{201703_fracastoro_methods,
  bibtex_show = {true},
  title       = {Methods and {Apparatuses} for {Encoding} and {Decoding} {Digital} 
                 {Images} {Through} {Superpixels}},
  assignee    = {Sisvel Technology Srl; Politecnico Di Torino},
  number      = {WO2017051358 (A1)},
  author      = {Fracastoro, Giulia and Magli, Enrico and Verdoja, Francesco and 
                 Grangetto, Marco},
  month       = mar,
  year        = {2017},
  funding     = {sisvel},
  abstract    = {A method and an apparatus for encoding and/or decoding digital
                 images or video streams are provided, wherein the encoding
                 apparatus includes a processor configured for reading at least
                 a portion of the image, segmenting the portion of the image in
                 order to obtain groups of pixels identified by borders
                 information and containing at least two pixels having one or
                 more homogeneous characteristics, computing, for each group of
                 pixels, a weight map on the basis of the borders information
                 associated to the group of pixels, a graph transform matrix on
                 the basis of the weight map, and transform coefficients on the
                 basis of the graph transform matrix (U) and of the pixels
                 contained in the group of pixels.},
  pdf         = {https://patentimages.storage.googleapis.com/f4/8a/16/bab6bd298dea8d/US20180278957A1.pdf},
  preview     = {patent_17.png}
}

@inproceedings{201703_verdoja_directional,
  address     = {New Orleans, LA},
  bibtex_show = {true},
  title       = {Directional graph weight prediction for image compression},
  doi         = {10.1109/ICASSP.2017.7952410},
  booktitle   = {2017 {IEEE} {Int.}\ {Conf.}\ on {Acoustics}, 
                 {Speech} and {Signal} {Processing} ({ICASSP})},
  author      = {Verdoja, Francesco and Grangetto, Marco},
  month       = mar,
  year        = {2017},
  pages       = {1517--1521},
  funding     = {sisvel},
  abstract    = {Graph-based models have recently attracted attention for their
                 potential to enhance transform coding image compression thanks
                 to their capability to efficiently represent discontinuities.
                 Graph transform gets closer to the optimal KLT by using weights
                 that represent inter-pixel correlations but the extra cost to
                 provide such weights can overwhelm the gain, especially in the
                 case of natural images rich of details. In this paper we
                 provide a novel idea to make graph transform adaptive to the
                 actual image content, avoiding the need to encode the graph
                 weights as side information. We show that an approach similar
                 to spatial prediction can be used to effectively predict graph
                 weights in place of pixels; in particular, we propose the
                 design of directional graph weight prediction modes and show
                 the resulting coding gain. The proposed approach can be used
                 jointly with other graph based intra prediction methods to
                 further enhance compression. Our comparative experimental
                 analysis, carried out with a fully fledged still image coding
                 prototype, shows that we are able to achieve significant coding
                 gains.},
  pdf         = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952410},
  abbr        = {ICASSP},
  preview     = {icassp_2_17.png}
}

@inproceedings{201703_verdoja_efficient,
  address     = {New Orleans, LA},
  bibtex_show = {true},
  title       = {Efficient representation of segmentation contours using chain 
                 codes},
  doi         = {10.1109/ICASSP.2017.7952399},
  booktitle   = {2017 {IEEE} {Int.}\ {Conf.}\ on {Acoustics}, 
                 {Speech} and {Signal} {Processing} ({ICASSP})},
  author      = {Verdoja, Francesco and Grangetto, Marco},
  month       = mar,
  year        = {2017},
  pages       = {1462--1466},
  funding     = {sisvel},
  abstract    = {Segmentation is one of the most important low-level tasks in 
                 image processing as it enables many higher level computer
                 vision tasks like object recognition and tracking. Segmentation
                 can also be exploited for image compression using recent
                 graph-based algorithms, provided that the corresponding
                 contours can be represented efficiently. Transmission of
                 borders is also key to distributed computer vision. In this
                 paper we propose a new chain code tailored to compress
                 segmentation contours. Based on the widely known 3OT, our
                 algorithm is able to encode regions avoiding borders it has
                 already coded once and without the need of any starting point
                 information for each region. We tested our method against three
                 other state of the art chain codes over the BSDS500 dataset,
                 and we demonstrated that the proposed chain code achieves the
                 highest compression ratio, resulting on average in over 27%
                 bit-per-pixel saving.},
  pdf         = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952399},
  abbr        = {ICASSP},
  preview     = {icassp_1_17.png}
}

@inproceedings{201707_verdoja_fast,
  address     = {Hong Kong},
  bibtex_show = {true},
  title       = {Fast 3D point cloud segmentation using supervoxels with geometry 
                 and color for 3D scene understanding},
  doi         = {10.1109/ICME.2017.8019382},
  booktitle   = {2017 {IEEE} {Int.}\ {Conf.}\ on {Multimedia} and 
                 {Expo} ({ICME})},
  author      = {Verdoja, Francesco and Thomas, Diego and Sugimoto, Akihiro},
  month       = jul,
  year        = {2017},
  pages       = {1285--1290},
  funding     = {sisvel},
  abstract    = {Segmentation of 3D colored point clouds is a research field
                 with renewed interest thanks to recent availability of
                 inexpensive consumer RGB-D cameras and its importance as an
                 unavoidable low-level step in many robotic applications.
                 However, 3D data's nature makes the task challenging and, thus,
                 many different techniques are being proposed , all of which
                 require expensive computational costs. This paper presents a
                 novel fast method for 3D colored point cloud segmen-tation. It
                 starts with supervoxel partitioning of the cloud, i.e., an
                 oversegmentation of the points in the cloud. Then it leverages
                 on a novel metric exploiting both geometry and color to
                 iteratively merge the supervoxels to obtain a 3D segmentation
                 where the hierarchical structure of partitions is maintained.
                 The algorithm also presents computational complexity linear to
                 the size of the input. Experimental results over two publicly
                 available datasets demonstrate that our proposed method
                 outperforms state-of-the-art techniques.},
  pdf         = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019382},
  code        = {https://github.com/fverdoja/Fast-3D-Pointcloud-Segmentation},
  abbr        = {ICME},
  preview     = {icme_17.png}
}

@phdthesis{201707_verdoja_use,
  address     = {Torino, Italy},
  type        = {Doctoral dissertation},
  bibtex_show = {true},
  title       = {The use of {Graph} {Fourier} {Transform} in image processing: a 
                 new solution to classical problems},
  shorttitle  = {The use of {Graph} {Fourier} {Transform} in image processing},
  school      = {Università degli Studi di Torino},
  author      = {Verdoja, Francesco},
  month       = july,
  year        = {2017},
  funding     = {sisvel},
  abstract    = {Graph-based approaches have recently seen a spike of interest 
                 in the image processing and computer vision communities, and
                 many classical problems are finding new solutions thanks to
                 these techniques. The Graph Fourier Transform (GFT), the
                 equivalent of the Fourier transform for graph signals, is used
                 in many domains to analyze and process data modeled by a graph.
                 In this thesis we present some classical image processing
                 problems that can be solved through the use of GFT. We'll focus
                 our attention on two main research area: the first is image
                 compression, where the use of the GFT is finding its way in
                 recent literature; we'll propose two novel ways to deal with
                 the problem of graph weight encoding. We'll also propose
                 approaches to reduce overhead costs of shape-adaptive
                 compression methods. The second research field is image anomaly
                 detection, GFT has never been proposed to this date to solve
                 this class of problems; we'll discuss here a novel technique
                 and we'll test its application on hyperspectral and medical
                 images. We'll show how graph approaches can be used to
                 generalize and improve performance of the widely popular RX
                 Detector, by reducing its computational complexity while at the
                 same time fixing the well known problem of its dependency from
                 covariance matrix estimation and inversion. All our experiments
                 confirm that graph-based approaches leveraging on the GFT can
                 be a viable option to solve tasks in multiple image processing
                 domains.},
  pdf         = {https://iris.unito.it/retrieve/handle/2318/1645345/352523/PhdThesis_Verdoja.pdf},
  preview     = {thesis_17.png}
}

@patent{201809_grangetto_method,
  bibtex_show = {true},
  title       = {Method and {Apparatus} for {Encoding} and {Decoding} {Digital} 
                 {Images} or {Video} {Streams}},
  assignee    = {Sisvel Technology S.r.l},
  number      = {WO2018158735 (A1)},
  author      = {Grangetto, Marco and Verdoja, Francesco},
  month       = sep,
  year        = {2018},
  funding     = {sisvel},
  abstract    = {A method for encoding digital images or video streams, 
                 includes a receiving phase, wherein a portion of an image is
                 received; a graph weights prediction phase, wherein the
                 elements of a weights matrix associated to the graph related to
                 the blocks of the image (predicted blocks) are predicted on the
                 basis of reconstructed, de-quantized and inverse-transformed
                 pixel values of at least one previously coded block (predictor
                 block) of the image, the weights matrix being a matrix
                 comprising elements denoting the level of similarity between a
                 pair of pixels composing said image, a graph transform
                 computation phase, wherein the graph Fourier transform of the
                 blocks of the image is performed, obtaining for the blocks a
                 set of coefficients determined on the basis of the predicted
                 weights; a coefficients quantization phase, wherein the
                 coefficients are quantized an output phase wherein a bitstream
                 comprising the transformed and quantized coefficients is
                 transmitted and/or stored.},
  pdf         = {https://patentimages.storage.googleapis.com/22/35/dc/d31d103dd9f3dd/US20200014955A1.pdf},
  preview     = {patent_2_18.png}
}

@patent{201809_grangetto_methods,
  bibtex_show = {true},
  title       = {Methods and {Apparatuses} for {Encoding} and {Decoding} 
                 {Superpixel} {Borders}},
  assignee    = {Sisvel Technology S.r.l},
  number      = {WO2018158738 (A1)},
  author      = {Grangetto, Marco and Verdoja, Francesco},
  month       = sep,
  year        = {2018},
  funding     = {sisvel},
  abstract    = {The present invention relates to a method for encoding the 
                 borders of pixel regions of an image, wherein the borders
                 contain a sequence of vertices subdividing the image into
                 regions of pixels (superpixels), by generating a sequence of
                 symbols from an alphabet including the step of: defining for
                 each superpixel a first vertex for coding the borders of the
                 superpixel according to a criterion common to all superpixels;
                 defining for each superpixel the same coding order of the
                 border vertices, either clockwise or counter-clockwise;
                 defining the order for coding the superpixels on the base of a
                 common rule depending on the relative positions of the first
                 vertices; defining a set of vertices as a known border, wherein
                 the following steps are performed for selecting a symbol of the
                 alphabet, for encoding the borders of the superpixels: a)
                 determining the first vertex of the next superpixel border
                 individuated by the common criterion; b) determining the next
                 vertex to be encoded on the basis of the coding direction; c)
                 selecting a first symbol (“0”) for encoding the next vertex if
                 the next vertex of a border pertains to the known border, d)
                 selecting a symbol (“1”; “2”) different from the first symbol
                 (“0”) if the next vertex is not in the known border; e)
                 repeating steps b), c), d) and e) until all vertices of the
                 superpixel border have been encoded; f) adding each vertex of
                 the superpixel border that was not in the known border to the
                 set; g) determining the next superpixel whose border is to be
                 encoded according to the common rule, if any; i) repeating
                 steps a)-g) until the borders of all the superpixels of the
                 image have being added to the known border.},
  pdf         = {https://patentimages.storage.googleapis.com/f2/f7/12/50250a322ba55b/US10708601.pdf},
  preview     = {patent_1_18.png}
}

@inproceedings{201810_lundell_hallucinating,
  address     = {Madrid, Spain},
  bibtex_show = {true},
  title       = {Hallucinating robots: {Inferring} obstacle distances from partial 
                 laser measurements},
  doi         = {10.1109/IROS.2018.8594399},
  booktitle   = {2018 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                 {Intelligent} {Robots} and {Systems} ({IROS})},
  author      = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
  month       = oct,
  year        = {2018},
  pages       = {4781--4787},
  funding     = {rose},
  abstract    = {Many mobile robots rely on 2D laser scanners for localization, 
                 mapping, and navigation. However, those sensors are unable to
                 correctly provide distance to obstacles such as glass panels
                 and tables whose actual occupancy is invisible at the height
                 the sensor is measuring. In this work, instead of estimating
                 the distance to obstacles from richer sensor readings such as
                 3D lasers or RGBD sensors, we present a method to estimate the
                 distance directly from raw 2D laser data. To learn a mapping
                 from raw 2D laser distances to obstacle distances we frame the
                 problem as a learning task and train a neural network formed as
                 an autoencoder. A novel configuration of network
                 hyperparameters is proposed for the task at hand and is
                 quantitatively validated on a test set. Finally, we
                 qualitatively demonstrate in real time on a Care-O-bot 4 that
                 the trained network can successfully infer obstacle distances
                 from partial 2D laser readings.},
  arxiv       = {1805.12338},
  pdf         = {https://arxiv.org/pdf/1805.12338},
  code        = {https://github.com/jsll/IROS2018-Hallucinating-Robots},
  video       = {https://www.youtube.com/watch?v=J5GOfbPTH88},
  abbr        = {IROS},
  preview     = {iros_18.jpg}
}

@inproceedings{201910_verdoja_deep,
  address     = {Toronto, Canada},
  bibtex_show = {true},
  title       = {Deep {Network} {Uncertainty} {Maps} for {Indoor} {Navigation}},
  doi         = {10.1109/Humanoids43949.2019.9035016},
  booktitle   = {2019 {IEEE-RAS} {Int.}\ {Conf.}\ on {Humanoid} {Robots} 
                 ({Humanoids})},
  author      = {Verdoja, Francesco and Lundell, Jens and Kyrki, Ville},
  month       = oct,
  year        = {2019},
  pages       = {112--119},
  funding     = {rose},
  abstract    = {Most mobile robots for indoor use rely on 2D laser scanners for
                 localization, mapping and navigation. These sensors, however,
                 cannot detect transparent surfaces or measure the full
                 occupancy of complex objects such as tables. Deep Neural
                 Networks have recently been proposed to overcome this
                 limitation by learning to estimate object occupancy. These
                 estimates are nevertheless subject to uncertainty, making the
                 evaluation of their confidence an important issue for these
                 measures to be useful for autonomous navigation and mapping. In
                 this work we approach the problem from two sides. First we
                 discuss uncertainty estimation in deep models, proposing a
                 solution based on a fully convolutional neural network. The
                 proposed architecture is not restricted by the assumption that
                 the uncertainty follows a Gaussian model, as in the case of
                 many popular solutions for deep model uncertainty estimation,
                 such as Monte-Carlo Dropout. We present results showing that
                 uncertainty over obstacle distances is actually better modeled
                 with a Laplace distribution. Then, we propose a novel approach
                 to build maps based on Deep Neural Network uncertainty models.
                 In particular, we present an algorithm to build a map that
                 includes information over obstacle distance estimates while
                 taking into account the level of uncertainty in each estimate.
                 We show how the constructed map can be used to increase global
                 navigation safety by planning trajectories which avoid areas of
                 high uncertainty, enabling higher autonomy for mobile robots in
                 indoor settings.},
  arxiv       = {1809.04891v3},
  pdf         = {https://arxiv.org/pdf/1809.04891v3},
  code        = {https://github.com/aalto-intelligent-robotics/uncertain_turtlebot_navigation},
  video       = {https://www.youtube.com/watch?v=J5GOfbPTH88},
  abbr        = {Humanoids},
  preview     = {humanoids_19.jpg}
}

@inproceedings{201911_lundell_robust,
  address     = {Macau, China},
  bibtex_show = {true},
  title       = {Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} 
                 {Completions}},
  doi         = {10.1109/IROS40897.2019.8967816},
  booktitle   = {2019 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                 {Intelligent} {Robots} and {Systems} ({IROS})},
  author      = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
  month       = nov,
  year        = {2019},
  pages       = {1526--1532},
  funding     = {rose},
  abstract    = {We present a method for planning robust grasps over uncertain
                 shape completed objects. For shape completion, a deep neural
                 network is trained to take a partial view of the object as
                 input and outputs the completed shape as a voxel grid. The key
                 part of the network is dropout layers which are enabled not
                 only during training but also at run-time to generate a set of
                 shape samples representing the shape uncertainty through Monte
                 Carlo sampling. Given the set of shape completed objects, we
                 generate grasp candidates on the mean object shape but evaluate
                 them based on their joint performance in terms of analytical
                 grasp metrics on all the shape candidates. We experimentally
                 validate and benchmark our method against another
                 state-of-the-art method with a Barrett hand on 90000 grasps in
                 simulation and 200 grasps on a real Franka Emika Panda. All
                 experimental results show statistically significant
                 improvements both in terms of grasp quality metrics and grasp
                 success rate, demonstrating that planning
                 shape-uncertainty-aware grasps brings significant advantages
                 over solely planning on a single shape estimate, especially
                 when dealing with complex or unknown objects.},
  arxiv       = {1903.00645},
  pdf         = {https://arxiv.org/pdf/1903.00645},
  code        = {https://github.com/aalto-intelligent-robotics/shape_completion_network},
  abbr        = {IROS},
  preview     = {iros_19.jpg}
}

@article{202001_verdoja_graph,
  bibtex_show = {true},
  title       = {Graph {Laplacian} for image anomaly detection},
  volume      = {31},
  issue       = {1},
  doi         = {10.1007/s00138-020-01059-4},
  number      = {11},
  journal     = {Machine Vision and Applications},
  author      = {Verdoja, Francesco and Grangetto, Marco},
  month       = jan,
  year        = {2020},
  funding     = {sisvel},
  abstract    = {Reed-Xiaoli detector (RXD) is recognized as the benchmark 
                 algorithm for image anomaly detection; however, it presents
                 known limitations, namely the dependence over the image
                 following a multivariate Gaussian model, the estimation and
                 inversion of a high-dimensional covariance matrix, and the
                 inability to effectively include spatial awareness in its
                 evaluation. In this work, a novel graph-based solution to the
                 image anomaly detection problem is proposed; leveraging the
                 graph Fourier transform, we are able to overcome some of RXD's
                 limitations while reducing computational cost at the same time.
                 Tests over both hyperspectral and medical images, using both
                 synthetic and real anomalies, prove the proposed technique is
                 able to obtain significant gains over performance by other
                 algorithms in the state of the art.},
  pdf         = {https://link.springer.com/content/pdf/10.1007/s00138-020-01059-4.pdf},
  code        = {https://github.com/fverdoja/LAD-Laplacian-Anomaly-Detector},
  abbr        = {MVAP},
  preview     = {mvap_20.jpg}
}

@inproceedings{202005_lundell_beyond,
  bibtex_show = {true},
  title       = {Beyond {Top}-{Grasps} {Through} {Scene} {Completion}},
  doi         = {10.1109/ICRA40945.2020.9197320},
  booktitle   = {2020 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                 ({ICRA})},
  author      = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
  month       = may,
  year        = {2020},
  pages       = {545--551},
  funding     = {rose},
  abstract    = {Current end-to-end grasp planning methods propose grasps in the
                 order of seconds that attain high grasp success rates on a
                 diverse set of objects, but often by constraining the workspace
                 to top-grasps. In this work, we present a method that allows
                 end-to-end top-grasp planning methods to generate full
                 six-degree-of-freedom grasps using a single RGBD view as input.
                 This is achieved by estimating the complete shape of the object
                 to be grasped, then simulating different viewpoints of the
                 object, passing the simulated viewpoints to an end-to-end grasp
                 generation method, and finally executing the overall best
                 grasp. The method was experimentally validated on a Franka
                 Emika Panda by comparing 429 grasps generated by the
                 state-of-the-art Fully Convolutional Grasp Quality CNN, both on
                 simulated and real camera images. The results show
                 statistically significant improvements in terms of grasp
                 success rate when using simulated images over real camera
                 images, especially when the real camera viewpoint is angled.},
  arxiv       = {1909.12908},
  pdf         = {https://arxiv.org/pdf/1909.12908},
  video       = {https://www.youtube.com/watch?v=Pret0HNU5us},
  abbr        = {ICRA},
  preview     = {icra_20.jpg}
}

@online{202005_verdoja_potential,
  bibtex_show = {true},
  title       = {On the {Potential} of {Smarter} {Multi}-layer {Maps}},
  url         = {https://mit-spark.github.io/PAL-ICRA2020},
  author      = {Verdoja, Francesco and Kyrki, Ville},
  month       = may,
  year        = {2020},
  note        = {Presented at the ``Perception, Action, Learning (PAL)'' workshop 
                 at the 2020 IEEE Int.\ Conf.\ on Robotics and Automation (ICRA)},
  funding     = {rose},
  abstract    = {The most common way for robots to handle environmental 
                 information is by using maps. At present, each kind of data is
                 hosted on a separate map, which complicates planning because a
                 robot attempting to perform a task needs to access and process
                 information from many different maps. Also, most often
                 correlation among the information contained in maps obtained
                 from different sources is not evaluated or exploited. In this
                 paper, we argue that in robotics a shift from single-source
                 maps to a multi-layer mapping formalism has the potential to
                 revolutionize the way robots interact with knowledge about
                 their environment. This observation stems from the raise in
                 metric-semantic mapping research, but expands to include in its
                 formulation also layers containing other information sources,
                 e.g., people flow, room semantic, or environment topology. Such
                 multi-layer maps, here named hypermaps, not only can ease
                 processing spatial data information but they can bring added
                 benefits arising from the interaction between maps. We imagine
                 that a new research direction grounded in such multi-layer
                 mapping formalism for robots can use artificial intelligence to
                 process the information it stores to present to the robot
                 task-specific information simplifying planning and bringing us
                 one step closer to high-level reasoning in robots.},
  arxiv       = {2005.11094},
  pdf         = {http://arxiv.org/pdf/2005.11094},
  video       = {https://www.youtube.com/watch?v=s6LWzBsc7Ao},
  abbr        = {ICRA},
  preview     = {icra_w_20.png}
}

@inproceedings{202009_zaenker_hypermap,
  bibtex_show = {true},
  title       = {Hypermap {Mapping} {Framework} and its {Application} to 
                 {Autonomous} {Semantic} {Exploration}},
  doi         = {10.1109/MFI49285.2020.9235231},
  booktitle   = {2020 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                 {Integration} for {Intelligent} {Systems} ({MFI})},
  author      = {Zaenker, Tobias and Verdoja, Francesco and Kyrki, Ville},
  month       = sep,
  year        = {2020},
  pages       = {133--139},
  funding     = {rose},
  abstract    = {Modern intelligent and autonomous robotic applications often 
                 require robots to have more information about their environment
                 than that provided by traditional occupancy grid maps. For
                 example, a robot tasked to perform autonomous semantic
                 exploration has to label objects in the environment it is
                 traversing while autonomously navigating. To solve this task
                 the robot needs to at least maintain an occupancy map of the
                 environment for navigation, an exploration map keeping track of
                 which areas have already been visited, and a semantic map where
                 locations and labels of objects in the environment are
                 recorded. As the number of maps required grows, an application
                 has to know and handle different map representations, which can
                 be a burden.We present the Hypermap framework, which can manage
                 multiple maps of different types. In this work, we explore the
                 capabilities of the framework to handle occupancy grid layers
                 and semantic polygonal layers, but the framework can be
                 extended with new layer types in the future. Additionally, we
                 present an algorithm to automatically generate semantic layers
                 from RGB-D images. We demonstrate the utility of the framework
                 using the example of autonomous exploration for semantic
                 mapping.},
  arxiv       = {1909.09526},
  pdf         = {https://arxiv.org/pdf/1909.09526},
  code        = {https://github.com/Eruvae/hypermap},
  abbr        = {MFI},
  preview     = {mfi_20.png}
}

@techreport{202100_kyrki_robots,
  bibtex_show = {true},
  title       = {Robots and the {Future} of {Welfare} {Services}: {A} {Finnish} 
                 {Roadmap}},
  shorttitle  = {Robots and the {Future} of {Welfare} {Services}},
  number      = {4},
  institution = {Aalto University},
  author      = {Kyrki, Ville and Aaltonen, Iina and Ainasoja, Antti and 
                 Heikkilä, Päivi and Heikkinen, Sari and Hennala, Lea and 
                 Koistinen, Pertti and Kämäräinen, Joni and Laakso, Kalle and 
                 Laitinen, Arto and Lammi, Hanna and Lanne, Marinka and 
                 Lappalainen, Inka and Lehtinen, Hannu and Lehto, Paula and 
                 Leppälahti, Teppo and Lundell, Jens and Melkas, Helinä and 
                 Niemelä, Marketta and Parjanen, Satu and Parviainen, Jaana and 
                 Pekkarinen, Satu and Pirhonen, Jari and Porokuokka, Jaakko and 
                 Rantanen, Teemu and Ruohomäki, Ismo and Saurio, Riika and 
                 Sahlgren, Otto and Särkikoski, Tuomo and Talja, Heli and 
                 Tammela, Antti and Tuisku, Outi and Turja, Tuuli and Aerschot, 
                 Lina Van and Verdoja, Francesco and Välimäki, Kari},
  year        = {2021},
  pages       = {72},
  funding     = {rose},
  abstract    = {This roadmap summarises a six-year multidisciplinary research 
                 project called Robots and the Future of Welfare Services
                 (ROSE), funded by the Strategic Research Council (SRC)
                 established within the Academy of Finland. The objective of the
                 project was to study the current and expected technical
                 opportunities and applications of robotics in welfare services,
                 particularly in care services for older people. The research
                 was carried out at three levels: individual, organisational and
                 societal. The roadmap provides highlights of the various
                 research activities of ROSE. We have studied the perspectives
                 of older adults and care professionals as users of robots, how
                 care organisations are able to adopt and utilise robots in
                 their services, how technology companies find robots as
                 business opportunity, and how the care robotics innovation
                 ecosystem is evolving. Based on these and other studies, we
                 evaluate the development and use of robots in care for older
                 adults in terms of social, ethical-philosophical and political
                 impacts as well as the public discussion on care robots. It
                 appears that there are many single- or limited-purpose robot
                 applications already commercially available in care services
                 for older adults. To be widely adopted, robots should still
                 increase maturity to be able to meet the requirements of care
                 environments, such as in terms of their ability to move in
                 smaller crowded spaces, easy and natural user interaction, and
                 task flexibility. The roadmap provides visions of what could be
                 technically expected in five and ten years. However, at the
                 same time, organisations' capabilities of adopting new
                 technology and integrating it into services should be supported
                 for them to be able to realise the potential of robots for the
                 benefits of care workers and older persons, as well as the
                 whole society. This roadmap also provides insight into the
                 wider impacts and risks of robotization in society and how to
                 steer it in a responsible way, presented as eight policy
                 recommendations. We also discuss the ROSE project research as a
                 multidisciplinary activity and present lessons learnt.},
  pdf         = {https://aaltodoc.aalto.fi/bitstream/123456789/107147/1/isbn9789526403236.pdf},
  preview     = {rose_21.png}
}

@article{202104_nguyen_le_probabilistic,
  bibtex_show = {true},
  title       = {Probabilistic {Surface} {Friction} {Estimation} {Based} on {Visual} 
                 and {Haptic} {Measurements}},
  volume      = {6},
  doi         = {10.1109/LRA.2021.3062585},
  number      = {2},
  journal     = {IEEE Robotics and Automation Letters},
  author      = {Nguyen Le, Tran and Verdoja, Francesco and Abu-Dakka, Fares J. and 
                 Kyrki, Ville},
  month       = apr,
  year        = {2021},
  pages       = {2838--2845},
  funding     = {rose}
}

@inproceedings{202105_lundell_multi-fingan,
  bibtex_show = {true},
  title       = {Multi-{FinGAN}: {Generative} {Coarse}-{To}-{Fine} {Sampling} of 
                 {Multi}-{Finger} {Grasps}},
  shorttitle  = {Multi-{FinGAN}},
  doi         = {10.1109/ICRA48506.2021.9561228},
  booktitle   = {2021 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                 ({ICRA})},
  author      = {Lundell, Jens and Corona, Enric and Nguyen Le, Tran and Verdoja,
                 Francesco and Weinzaepfel, Philippe and Rogez, Grégory and
                 Moreno-Noguer, Francesc and Kyrki, Ville},
  month       = may,
  year        = {2021},
  pages       = {4495--4501},
  funding     = {rose}
}

@online{202107_verdoja_notes,
  bibtex_show = {true},
  title       = {Notes on the {Behavior} of {MC} {Dropout}},
  url         = {https://sites.google.com/view/udlworkshop2021},
  author      = {Verdoja, Francesco and Kyrki, Ville},
  month       = jul,
  year        = {2021},
  note        = {Presented at the ``Uncertainty and Robustness in Deep
                 Learning (UDL)'' workshop at the 2021 Int.\ Conf.\ on Machine 
                 Learning (ICML)},
  funding     = {rose}
}

@inproceedings{202108_dengler_online,
  bibtex_show = {true},
  title       = {Online {Object}-{Oriented} {Semantic} {Mapping} and {Map} 
                 {Updating}},
  doi         = {10.1109/ECMR50962.2021.9568817},
  booktitle   = {2021 {Eur.} {Conf.} on {Mobile} {Robots} ({ECMR})},
  author      = {Dengler, Nils and Zaenker, Tobias and Verdoja, Francesco and 
                 Bennewitz, Maren},
  month       = aug,
  year        = {2021},
  funding     = {rose}
}

@article{202110_lundell_ddgc,
  bibtex_show = {true},
  title       = {{DDGC}: {Generative} {Deep} {Dexterous} {Grasping} in 
                 {Clutter}},
  volume      = {6},
  shorttitle  = {{DDGC}},
  doi         = {10.1109/LRA.2021.3096239},
  number      = {4},
  journal     = {IEEE Robotics and Automation Letters},
  author      = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
  month       = oct,
  year        = {2021},
  pages       = {6899--6906},
  funding     = {rose}
}

@inproceedings{202112_kinnari_gnss-denied,
  bibtex_show = {true},
  title       = {{GNSS}-denied geolocalization of {UAVs} by visual matching of 
                 onboard camera images with orthophotos},
  doi         = {10.1109/ICAR53236.2021.9659333},
  booktitle   = {2021 {IEEE} {Int.} {Conf.} on {Advanced} {Robotics} ({ICAR})},
  author      = {Kinnari, Jouko and Verdoja, Francesco and Kyrki, Ville},
  month       = dec,
  year        = {2021},
  pages       = {555--562},
  funding     = {saab}
}

@inproceedings{202208_sivananda_augmented,
  bibtex_show = {true},
  title       = {Augmented {Environment} {Representations} with {Complete} 
                 {Object} {Models}},
  doi         = {10.1109/RO-MAN53752.2022.9900516},
  booktitle   = {2022 {IEEE} {Int.}\ {Conf.}\ on {Robot} and 
                 {Human} {Interactive} {Communication} ({RO}-{MAN})},
  author      = {Sivananda, Krishnananda Prabhu and Verdoja, Francesco and Kyrki, 
                 Ville},
  month       = aug,
  year        = {2022},
  pages       = {1123--1130}
}

@online{202208_verdoja_generating,
  bibtex_show = {true},
  title       = {Generating people flow from architecture of real unseen 
                 environments},
  url         = {https://iros2022-pnarude.github.io},
  author      = {Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville},
  month       = aug,
  year        = {2022},
  note        = {Presented at the ``Perception and Navigation for Autonomous Robotics
                 in Unstructured and Dynamic Environments (PNARUDE)'' workshop at the
                 IEEE/RSJ Int.\ Conf.\ on Intelligent Robots and Systems (IROS)},
  funding     = {santtu}
}

@article{202210_kinnari_season-invariant,
  bibtex_show = {true},
  title       = {Season-{Invariant} {GNSS}-{Denied} {Visual} {Localization} for 
                 {UAVs}},
  volume      = {7},
  doi         = {10.1109/LRA.2022.3191038},
  number      = {4},
  journal     = {IEEE Robotics and Automation Letters},
  author      = {Kinnari, Jouko and Verdoja, Francesco and Kyrki, Ville},
  month       = oct,
  year        = {2022},
  pages       = {10232--10239},
  code        = {https://github.com/aalto-intelligent-robotics/sivl},
  award       = {Awarded by the IEEE Finland CSS/RAS/SMCS Joint Chapter},
  award_name  = {2023 Best Paper Award},
  funding     = {saab}
}

@article{202309_kucner_survey,
  bibtex_show = {true},
  title       = {Survey of maps of dynamics for mobile robots},
  volume      = {42},
  doi         = {10.1177/02783649231190428},
  number      = {11},
  journal     = {The Int.\ Journal of Robotics Research},
  author      = {Kucner, Tomasz Piotr and Magnusson, Martin and Mghames, Sariah and
                 Palmieri, Luigi and Verdoja, Francesco and Swaminathan, 
                 Chittaranjan Srinivas and Krajník, Tomáš and Schaffernicht, Erik 
                 and Bellotto, Nicola and Hanheide, Marc and Lilienthal, Achim J},
  month       = sep,
  year        = {2023},
  pages       = {977--1006},
  funding     = {santtu}
}

@article{202310_kinnari_lsvl,
  bibtex_show = {true},
  title       = {{LSVL}: {Large}-scale season-invariant visual localization for 
                 {UAVs}},
  volume      = {168},
  doi         = {10.1016/j.robot.2023.104497},
  journal     = {Robotics and Autonomous Systems},
  author      = {Kinnari, Jouko and Renzulli, Riccardo and Verdoja, Francesco and 
                 Kyrki, Ville},
  month       = oct,
  year        = {2023},
  video       = {https://ars.els-cdn.com/content/image/1-s2.0-S0921889023001367-mmc1.mp4},
  funding     = {saab}
}

@inproceedings{202310_lundell_constrained,
  bibtex_show = {true},
  title       = {Constrained {Generative} {Sampling} of 6-{DoF} {Grasps}},
  doi         = {10.1109/IROS55552.2023.10341344},
  booktitle   = {2023 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots} and 
                 {Systems} ({IROS})},
  author      = {Lundell, Jens and Verdoja, Francesco and Nguyen Le, Tran and 
                 Mousavian, Arsalan and Fox, Dieter and Kyrki, Ville},
  month       = oct,
  year        = {2023},
  pages       = {2940--2946},
  code        = {https://github.com/jsll/position-contrained-6dof-graspnet}
}

@online{202405_chaubey_jointly,
  bibtex_show = {true},
  title       = {Jointly Learning Cost and Constraints from Demonstrations for Safe 
                 Trajectory Generation},
  url         = {https://sites.google.com/view/icra24-physical-hri},
  author      = {Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville},
  month       = may,
  year        = {2024},
  note        = {Presented at the ``Towards Collaborative Partners: Design, Shared 
                 Control, and Robot Learning for Physical Human-Robot Interaction 
                 (pHRI)'' workshop at the IEEE Int.\ Conf.\ on Robotics and 
                 Automation (ICRA)},
  funding     = {santtu}
}

@online{202405_pekkanen_evaluating,
  bibtex_show = {true},
  title       = {Evaluating the quality of robotic visual-language maps},
  url         = {https://vlmnm-workshop.github.io},
  author      = {Pekkanen, Matti and Mihaylova, Tsvetomila and Verdoja, Francesco 
                 and Kyrki, Ville},
  month       = may,
  year        = {2024},
  note        = {Presented at the ``Vision-Language Models for Navigation and 
                 Manipulation (VLMNM)'' workshop at the IEEE Int.\ Conf.\ on 
                 Robotics and Automation (ICRA)},
  funding     = {hypermaps, santtu}
}

@online{202405_pekkanen_modeling,
  bibtex_show = {true},
  title       = {Modeling movable objects improves localization in dynamic 
                 environments},
  url         = {https://construction-robots.github.io},
  author      = {Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville},
  month       = may,
  year        = {2024},
  note        = {Presented at the ``Future of Construction: Lifelong Learning Robots 
                 in Changing Construction Sites'' workshop at the IEEE Int.\ Conf.\ 
                 on Robotics and Automation (ICRA)},
  funding     = {santtu}
}

@online{202405_verdoja_using,
  bibtex_show = {true},
  title       = {Using occupancy priors to generalize people flow predictions},
  url         = {https://motionpredictionicra2024.github.io},
  author      = {Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville},
  month       = may,
  year        = {2024},
  note        = {Presented at the ``Long-term Human Motion Prediction'' workshop at 
                 the IEEE Int.\ Conf.\ on Robotics and Automation (ICRA)},
  funding     = {hypermaps}
}

@inproceedings{202409_pekkanen_localization,
  bibtex_show = {true},
  title       = {Localization under consistent assumptions over dynamics},
  booktitle   = {2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                 {Integration} for {Intelligent} {Systems} ({MFI})},
  author      = {Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville},
  month       = sep,
  year        = {2024},
  note        = {accepted},
  funding     = {santtu}
}

@inproceedings{202409_pekkanen_object,
  bibtex_show = {true},
  title       = {Object-oriented mapping in dynamic environments},
  booktitle   = {2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                 {Integration} for {Intelligent} {Systems} ({MFI})},
  author      = {Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville},
  month       = sep,
  year        = {2024},
  note        = {accepted},
  funding     = {santtu}
}

@inproceedings{202410_chaubey_jointly,
  bibtex_show = {true},
  title       = {Jointly Learning Cost and Constraints from Demonstrations for Safe 
                 Trajectory Generation},
  booktitle   = {2024 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots} and 
                 {Systems} ({IROS})},
  author      = {Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville},
  month       = oct,
  year        = {2024},
  note        = {accepted},
  funding     = {santtu}
}

@inproceedings{202410_verdoja_bayesian,
  bibtex_show = {true},
  title       = {Bayesian Floor Field: Transferring people flow predictions across
                 environments},
  url         = {https://arxiv.org/abs/2208.10851},
  booktitle   = {2024 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots} and 
                 {Systems} ({IROS})},
  author      = {Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville},
  month       = oct,
  year        = {2024},
  note        = {accepted},
  code        = {https://github.com/aalto-intelligent-robotics/bayesianfloorfield},
  funding     = {hypermaps}
}
