<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | francesco verdoja </title> <meta name="author" content="Francesco Verdoja"> <meta name="description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta name="keywords" content="robotics, mapping, computer vision, artificial intelligence, ai, machine learning, research, academia"> <meta property="og:site_name" content="francesco verdoja"> <meta property="og:type" content="website"> <meta property="og:title" content="francesco verdoja | publications"> <meta property="og:url" content="https://fverdoja.github.io/publications/"> <meta property="og:description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta property="og:image" content="assets/img/fra.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta name="twitter:image" content="assets/img/fra.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Francesco Verdoja"
        },
        "url": "https://fverdoja.github.io/publications/",
        "@type": "WebSite",
        "description": "Academy Research Fellow, Intelligent Robotics, Aalto University, Finland
",
        "headline": "publications",
        
        "sameAs": ["https://orcid.org/0000-0002-9551-6186", "https://scholar.google.com/citations?user=3DDM3_kAAAAJ", "https://www.scopus.com/authid/detail.uri?authorId=56685641600", "https://github.com/fverdoja", "https://www.linkedin.com/in/fverdoja", "https://research.aalto.fi/en/persons/francesco-verdoja"],
        
        "name": "Francesco Verdoja",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fverdoja.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">francesco</span> verdoja </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/events/">events </a> </li> <li class="nav-item "> <a class="nav-link" href="/funding/">funding </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"><a class="nav-link" href="/assets/pdf/verdoja_francesco.pdf">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p>Publications are sorted in reverse chronological order and grouped by type.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_2_24-480.webp 480w,/assets/img/publication_preview/iros_2_24-800.webp 800w,/assets/img/publication_preview/iros_2_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_2_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202410_chaubey_jointly" class="col-sm-8"> <div class="title">Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation</div> <div class="author"> Shivam Chaubey, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best SSRR Paper finalist</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.03491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2405.03491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://version.aalto.fi/gitlab/chaubes1/jointly-learning-cost-and-constraints/-/raw/main/images/video.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://version.aalto.fi/gitlab/chaubes1/jointly-learning-cost-and-constraints" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Nominated for the <em>Best Safety, Security, and Rescue Robotics Paper</em> award sponsored by the International Rescue System Initiative (IRSI) at the 2024 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</p> </div> <div class="abstract hidden"> <p>Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202410_chaubey_jointly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Jointly Learning Cost and Constraints from Demonstrations for
                   Safe Trajectory Generation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots}
                   and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_1_24-480.webp 480w,/assets/img/publication_preview/iros_1_24-800.webp 800w,/assets/img/publication_preview/iros_1_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_1_24.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_1_24.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202410_verdoja_bayesian" class="col-sm-8"> <div class="title">Bayesian Floor Field: Transferring people flow predictions across environments</div> <div class="author"> <em>Francesco Verdoja</em>, Tomasz Piotr Kucner, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.10851" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2208.10851" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=JQHTyouIDd0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/aalto-intelligent-robotics/bayesianfloorfield" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Mapping people dynamics is a crucial skill for robots, because it enables them to coexist in human-inhabited environments. However, learning a model of people dynamics is a time consuming process which requires observation of large amount of people moving in an environment. Moreover, approaches for mapping dynamics are unable to transfer the learned models across environments: each model is only able to describe the dynamics of the environment it has been built in. However, the impact of architectural geometry on people’s movement can be used to anticipate their patterns of dynamics, and recent work has looked into learning maps of dynamics from occupancy. So far however, approaches based on trajectories and those based on geometry have not been combined. In this work we propose a novel Bayesian approach to learn people dynamics able to combine knowledge about the environment geometry with observations from human trajectories. An occupancy-based deep prior is used to build an initial transition model without requiring any observations of pedestrian; the model is then updated when observations become available using Bayesian inference. We demonstrate the ability of our model to increase data efficiency and to generalize across real large-scale environments, which is unprecedented for maps of dynamics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202410_verdoja_bayesian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bayesian Floor Field: Transferring people flow predictions 
                   across environments}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2208.10851}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots} 
                   and 
                   {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_2_24-480.webp 480w,/assets/img/publication_preview/mfi_2_24-800.webp 800w,/assets/img/publication_preview/mfi_2_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mfi_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_2_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202409_pekkanen_localization" class="col-sm-8"> <div class="title">Localization Under Consistent Assumptions Over Dynamics</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.16702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2305.16702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Accurate maps are a prerequisite for virtually all mobile robot tasks. Most state-of-the-art maps assume a static world; therefore, dynamic objects are filtered out of the measurements. However, this division ignores movable but non- moving—i.e., semi-static—objects, which are usually recorded in the map and treated as static objects, violating the static world assumption and causing errors in the localization. This paper presents a method for consistently modeling moving and movable objects to match the map and measurements. This reduces the error resulting from inconsistent categorization and treatment of non-static measurements. A semantic segmentation network is used to categorize the measurements into static and semi-static classes, and a background subtraction filter is used to remove dynamic measurements. Finally, we show that consis- tent assumptions over dynamics improve localization accuracy when compared against a state-of-the-art baseline solution using real-world data from the Oxford Radar RobotCar data set.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202409_pekkanen_localization</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Localization {Under} {Consistent} {Assumptions} {Over}
                   {Dynamics}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI62651.2024.10705760}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_1_24-480.webp 480w,/assets/img/publication_preview/mfi_1_24-800.webp 800w,/assets/img/publication_preview/mfi_1_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mfi_1_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_1_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202409_pekkanen_object" class="col-sm-8"> <div class="title">Object-Oriented Grid Mapping in Dynamic Environments</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2309.08324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/lamide" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Grid maps, especially occupancy grid maps, are ubiquitous in many mobile robot applications. To simplify the process of learning the map, grid maps subdivide the world into a grid of cells whose occupancies are independently estimated using measurements in the perceptual field of the particular cell. However, the world consists of objects that span multiple cells, which means that measurements falling onto a cell provide evidence of the occupancy of other cells belonging to the same object. Current models do not capture this correlation and, therefore, do not use object-level information for estimating the state of the environment. In this work, we present a way to generalize the update of grid maps, relaxing the assumption of independence. We propose modeling the relationship between the measurements and the occupancy of each cell as a set of latent variables and jointly estimate those variables and the posterior of the map. We propose a method to estimate the latent variables by clustering based on semantic labels and an extension to the Normal Distributions Transform Occupancy Map (NDT-OM) to facilitate the proposed map update method. We perform comprehensive map creation and localization ex- periments with real-world data sets and show that the proposed method creates better maps in highly dynamic environments compared to state-of-the-art methods. Finally, we demonstrate the ability of the proposed method to remove occluded objects from the map in a lifelong map update scenario.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202409_pekkanen_object</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object-{Oriented} {Grid} {Mapping} in {Dynamic} 
                   {Environments}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI62651.2024.10705762}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">workshop articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_1_24-480.webp 480w,/assets/img/publication_preview/icra_1_24-800.webp 800w,/assets/img/publication_preview/icra_1_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_1_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_1_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202405_chaubey_jointly" class="col-sm-8"> <div class="title">Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation</div> <div class="author"> Shivam Chaubey, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Towards Collaborative Partners: Design, Shared Control, and Robot Learning for Physical Human-Robot Interaction (pHRI)” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/1DZcfnhbppEWc_fM9rWBgJyVI65q5ythz/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Learning from Demonstration (LfD) allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_chaubey_jointly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Jointly Learning Cost and Constraints from Demonstrations for 
                   Safe Trajectory Generation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://sites.google.com/view/icra24-physical-hri}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Towards Collaborative Partners: Design, 
                   Shared Control, and Robot Learning for Physical Human-Robot
                   Interaction (pHRI)'' workshop at the IEEE Int.\ Conf.\ on 
                   Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_2_24-480.webp 480w,/assets/img/publication_preview/icra_2_24-800.webp 800w,/assets/img/publication_preview/icra_2_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_2_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202405_pekkanen_evaluating" class="col-sm-8"> <div class="title">Evaluating the quality of robotic visual-language maps</div> <div class="author"> Matti Pekkanen, Tsvetomila Mihaylova, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Vision-Language Models for Navigation and Manipulation (VLMNM)” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=FspmwJHZ2G" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. In this paper, we propose a way to analyze the quality of maps created using VLMs by evaluating two critical properties: queryability and consistency. We demonstrate the proposed method by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_pekkanen_evaluating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluating the quality of robotic visual-language maps}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://vlmnm-workshop.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Mihaylova, Tsvetomila and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Vision-Language Models for Navigation and 
                   Manipulation (VLMNM)'' workshop at the IEEE Int.\ Conf.\ on 
                   Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_3_24-480.webp 480w,/assets/img/publication_preview/icra_3_24-800.webp 800w,/assets/img/publication_preview/icra_3_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_3_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_3_24.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202405_pekkanen_modeling" class="col-sm-8"> <div class="title">Modeling movable objects improves localization in dynamic environments</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Future of Construction: Lifelong Learning Robots in Changing Construction Sites” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://construction-robots.github.io/papers/49.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most state-of-the-art robotic maps assume a static world; therefore, dynamic objects are filtered out of the measurements. However, this division ignores movable but non- moving, i.e., semi-static objects, which are usually recorded in the map and treated as static objects, violating the static world assumption and causing errors in the localization. This paper presents a method for modeling moving and movable objects to match the map and measurements consistently. This reduces the error resulting from inconsistent categorization and treatment of non-static measurements. A semantic segmentation network is used to categorize the measurements into static and semi- static classes, and a background subtraction-based filtering method is used to remove dynamic measurements. Experimental comparison against a state-of-the-art baseline solution using real-world data from the Oxford Radar RobotCar data set shows that consistent assumptions over dynamics increase localization accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_pekkanen_modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling movable objects improves localization in dynamic 
                   environments}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://construction-robots.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Future of Construction: Lifelong Learning 
                   Robots in Changing Construction Sites'' workshop at the IEEE
                   Int.\ Conf.\ on Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_4_24-480.webp 480w,/assets/img/publication_preview/icra_4_24-800.webp 800w,/assets/img/publication_preview/icra_4_24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_4_24.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_4_24.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202405_verdoja_using" class="col-sm-8"> <div class="title">Using occupancy priors to generalize people flow predictions</div> <div class="author"> <em>Francesco Verdoja</em>, Tomasz Piotr Kucner, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Long-term Human Motion Prediction” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://motionpredictionicra2024.github.io/proceedings/lhmp2024_id02.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Mapping people dynamics is a crucial skill for robots, because it enables them to coexist in human-inhabited environments. However, learning a model of people dynamics is a time consuming process which requires observation of large amount of people moving in an environment. Moreover, approaches for mapping dynamics are unable to transfer the learned models across environments: each model is only able to describe the dynamics of the environment it has been built in. However, the impact of architectural geometry on people’s movement can be used to anticipate their patterns of dynamics, and recent work has looked into learning maps of dynamics from occupancy. So far however, approaches based on trajectories and those based on geometry have not been combined. In this work we propose a novel Bayesian approach to learn people dynamics able to combine knowledge about the environment geometry with observations from human trajectories. An occupancy-based deep prior is used to build an initial transition model without requiring any observations of pedestrian; the model is then updated when observations become available using Bayesian inference. We demonstrate the ability of our model to increase data efficiency and to generalize across real large-scale environments, which is unprecedented for maps of dynamics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_verdoja_using</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using occupancy priors to generalize people flow predictions}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://motionpredictionicra2024.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Long-term Human Motion Prediction'' workshop
                   at the IEEE Int.\ Conf.\ on Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <h3 class="bibliography">journal articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ras_23-480.webp 480w,/assets/img/publication_preview/ras_23-800.webp 800w,/assets/img/publication_preview/ras_23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ras_23.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ras_23.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202310_kinnari_lsvl" class="col-sm-8"> <div class="title">LSVL: Large-scale season-invariant visual localization for UAVs</div> <div class="author"> Jouko Kinnari, Riccardo Renzulli, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.03581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0921889023001367/pdfft?md5=f9adcc28337beda3aed23294f5cb5615&amp;pid=1-s2.0-S0921889023001367-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://ars.els-cdn.com/content/image/1-s2.0-S0921889023001367-mmc1.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Localization of autonomous unmanned aerial vehicles (UAVs) relies heavily on Global Navigation Satellite Systems (GNSS), which are susceptible to interference. Especially in security applications, robust localization algorithms independent of GNSS are needed to provide dependable operations of autonomous UAVs also in interfered conditions. Typical non-GNSS visual localization approaches rely on known starting pose, work only on a small-sized map, or require known flight paths before a mission starts. We consider the problem of localization with no information on initial pose or planned flight path. We propose a solution for global visual localization on a map at scale up to 100 km2, based on matching orthoprojected UAV images to satellite imagery using learned season-invariant descriptors. We show that the method is able to determine heading, latitude and longitude of the UAV at 12.6-18.7 m lateral translation error in as few as 23.2-44.4 updates from an uninformed initialization, also in situations of significant seasonal appearance difference (winter-summer) between the UAV image and the map. We evaluate the characteristics of multiple neural network architectures for generating the descriptors, and likelihood estimation methods that are able to provide fast convergence and low localization error. We also evaluate the operation of the algorithm using real UAV data and evaluate running time on a real-time embedded platform. We believe this is the first work that is able to recover the pose of an UAV at this scale and rate of convergence, while allowing significant seasonal difference between camera observations and map.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202310_kinnari_lsvl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LSVL}: {Large}-scale season-invariant visual localization for 
                   {UAVs}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{168}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.robot.2023.104497}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Autonomous Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kinnari, Jouko and Renzulli, Riccardo and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJRR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijrr_23-480.webp 480w,/assets/img/publication_preview/ijrr_23-800.webp 800w,/assets/img/publication_preview/ijrr_23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ijrr_23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijrr_23.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202309_kucner_survey" class="col-sm-8"> <div class="title">Survey of maps of dynamics for mobile robots</div> <div class="author"> Tomasz Piotr Kucner, Martin Magnusson, Sariah Mghames, Luigi Palmieri, <em>Francesco Verdoja</em>, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Chittaranjan Srinivas Swaminathan, Tomáš Krajník, Erik Schaffernicht, Nicola Bellotto, Marc Hanheide, Achim J Lilienthal' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">6 more authors</span> </div> <div class="periodical"> <em>The Int. Journal of Robotics Research</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://journals.sagepub.com/doi/epub/10.1177/02783649231190428" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robotic mapping provides spatial information for autonomous agents. Depending on the tasks they seek to enable, the maps created range from simple 2D representations of the environment geometry to complex, multilayered semantic maps. This survey article is about maps of dynamics (MoDs), which store semantic information about typical motion patterns in a given environment. Some MoDs use trajectories as input, and some can be built from short, disconnected observations of motion. Robots can use MoDs, for example, for global motion planning, improved localization, or human motion prediction. Accounting for the increasing importance of maps of dynamics, we present a comprehensive survey that organizes the knowledge accumulated in the field and identifies promising directions for future work. Specifically, we introduce field-specific vocabulary, summarize existing work according to a novel taxonomy, and describe possible applications and open research problems. We conclude that the field is mature enough, and we expect that maps of dynamics will be increasingly used to improve robot performance in real-world use cases. At the same time, the field is still in a phase of rapid development where novel contributions could significantly impact this research area.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202309_kucner_survey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Survey of maps of dynamics for mobile robots}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/02783649231190428}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Int.\ Journal of Robotics Research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kucner, Tomasz Piotr and Magnusson, Martin and Mghames, Sariah and Palmieri, Luigi and Verdoja, Francesco and Swaminathan, Chittaranjan Srinivas and Krajník, Tomáš and Schaffernicht, Erik and Bellotto, Nicola and Hanheide, Marc and Lilienthal, Achim J}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{977--1006}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_23-480.webp 480w,/assets/img/publication_preview/iros_23-800.webp 800w,/assets/img/publication_preview/iros_23-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_23.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_23.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202310_lundell_constrained" class="col-sm-8"> <div class="title">Constrained Generative Sampling of 6-DoF Grasps</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, Tran Nguyen Le, Arsalan Mousavian, Dieter Fox, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ville Kyrki' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">1 more author</span> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.10745" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2302.10745" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=kTZD6cxTtv8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/jsll/position-contrained-6dof-graspnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Most state-of-the-art data-driven grasp sampling methods propose stable and collision-free grasps uniformly on the target object. For bin-picking, executing any of those reachable grasps is sufficient. However, for completing specific tasks, such as squeezing out liquid from a bottle, we want the grasp to be on a specific part of the object’s body while avoiding other locations, such as the cap. This work presents a generative grasp sampling network, VCGS, capable of constrained 6 Degrees of Freedom (DoF) grasp sampling. In addition, we also curate a new dataset designed to train and evaluate methods for constrained grasping. The new dataset, called CONG, consists of over 14 million training samples of synthetically rendered point clouds and grasps at random target areas on 2889 objects. VCGS is benchmarked against GraspNet, a state-of-the-art unconstrained grasp sampler, in simulation and on a real robot. The results demonstrate that VCGS achieves a 10-15% higher grasp success rate than the baseline while being 2-3 times as sample efficient. Supplementary material is available on our project website.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202310_lundell_constrained</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Constrained {Generative} {Sampling} of 6-{DoF} {Grasps}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS55552.2023.10341344}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots} 
                   and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Nguyen Le, Tran and Mousavian, Arsalan and Fox, Dieter and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2940--2946}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <h3 class="bibliography">journal articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_22-480.webp 480w,/assets/img/publication_preview/ral_22-800.webp 800w,/assets/img/publication_preview/ral_22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ral_22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ral_22.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202210_kinnari_season-invariant" class="col-sm-8"> <div class="title">Season-Invariant GNSS-Denied Visual Localization for UAVs</div> <div class="author"> Jouko Kinnari, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">2023 Best Paper Award</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2110.01967" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9830867" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/sivl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Awarded by the IEEE Finland CSS/RAS/SMCS Joint Chapter</p> </div> <div class="abstract hidden"> <p>Localization without Global Navigation Satellite Systems (GNSS) is a critical functionality in autonomous operations of unmanned aerial vehicles (UAVs). Vision-based localization on a known map can be an effective solution, but it is burdened by two main problems: places have different appearance depending on weather and season, and the perspective discrepancy between the UAV camera image and the map make matching hard. In this work, we propose a localization solution relying on matching of UAV camera images to georeferenced orthophotos with a trained convolutional neural network model that is invariant to significant seasonal appearance difference (winter-summer) between the camera image and map. We compare the convergence speed and localization accuracy of our solution to six reference methods. The results show major improvements with respect to reference methods, especially under high seasonal variation. We finally demonstrate the ability of the method to successfully localize a real UAV, showing that the proposed method is robust to perspective changes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202210_kinnari_season-invariant</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Season-{Invariant} {GNSS}-{Denied} {Visual} {Localization} for 
                   {UAVs}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2022.3191038}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kinnari, Jouko and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10232--10239}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RO-MAN</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/roman_22-480.webp 480w,/assets/img/publication_preview/roman_22-800.webp 800w,/assets/img/publication_preview/roman_22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/roman_22.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="roman_22.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202208_sivananda_augmented" class="col-sm-8"> <div class="title">Augmented Environment Representations with Complete Object Models</div> <div class="author"> Krishnananda Prabhu Sivananda, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2022 IEEE Int. Conf. on Robot and Human Interactive Communication (RO-MAN)</em>, Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.07298" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2103.07298" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While 2D occupancy maps commonly used in mobile robotics enable safe navigation in indoor environments, in order for robots to understand and interact with their environment and its inhabitants representing 3D geometry and semantic environment information is required. Semantic information is crucial in effective interpretation of the meanings humans attribute to different parts of a space, while 3D geometry is important for safety and high-level understanding. We propose a pipeline that can generate a multi-layer representation of indoor environments for robotic applications. The proposed representation includes 3D metric-semantic layers, a 2D occupancy layer, and an object instance layer where known objects are replaced with an approximate model obtained through a novel model-matching approach. The metric-semantic layer and the object instance layer are combined to form an augmented representation of the environment. Experiments show that the proposed shape matching method outperforms a state-of-the-art deep learning method when tasked to complete unseen parts of objects in the scene. The pipeline performance translates well from simulation to real world as shown by F1-score analysis, with semantic segmentation accuracy using Mask R-CNN acting as the major bottleneck. Finally, we also demonstrate on a real robotic platform how the multi-layer map can be used to improve navigation safety.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202208_sivananda_augmented</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Augmented {Environment} {Representations} with {Complete} 
                   {Object} {Models}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/RO-MAN53752.2022.9900516}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 {IEEE} {Int.}\ {Conf.}\ on {Robot} and 
                   {Human} {Interactive} {Communication} ({RO}-{MAN})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sivananda, Krishnananda Prabhu and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1123--1130}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">workshop articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_22-480.webp 480w,/assets/img/publication_preview/iros_22-800.webp 800w,/assets/img/publication_preview/iros_22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_22.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202210_verdoja_generating" class="col-sm-8"> <div class="title">Generating people flow from architecture of real unseen environments</div> <div class="author"> <em>Francesco Verdoja</em>, Tomasz Piotr Kucner, and Ville Kyrki </div> <div class="periodical"> Oct 2022 </div> <div class="periodical"> Presented at the “Perception and Navigation for Autonomous Robotics in Unstructured and Dynamic Environments (PNARUDE)” workshop at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iros2022-pnarude.github.io/pdf/PNARUDE_IROS2022_Francesco_Verdoja_Generating_people_flow_from_architecture_of_real_unseen_environments.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in multi-fingered robotic grasping have enabled fast 6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in cluttered scenes, on the other hand, remains mostly unexplored due to the added difficulty of reasoning over obstacles which greatly increases the computational time to generate high-quality collision-free grasps. In this work we address such limitations by introducing DDGC, a fast generative multi-finger grasp sampling method that can generate high quality grasps in cluttered scenes from a single RGB-D image. DDGC is built as a network that encodes scene information to produce coarse-to-fine collision-free grasp poses and configurations. We experimentally benchmark DDGC against the simulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and 7 real world scenes. The results show that DDGC outperforms the baseline on synthesizing high-quality grasps and removing clutter while being 5 times faster. This, in turn, opens up the door for using multi-finger grasps in practical applications which has so far been limited due to the excessive computation time needed by other methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202210_verdoja_generating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating people flow from architecture of real unseen 
                   environments}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://iros2022-pnarude.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Perception and Navigation for Autonomous 
                   Robotics in Unstructured and Dynamic Environments (PNARUDE)''
                   workshop at the IEEE/RSJ Int.\ Conf.\ on Intelligent Robots and
                   Systems (IROS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <h3 class="bibliography">journal articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_2_21-480.webp 480w,/assets/img/publication_preview/ral_2_21-800.webp 800w,/assets/img/publication_preview/ral_2_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ral_2_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ral_2_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202110_lundell_ddgc" class="col-sm-8"> <div class="title">DDGC: Generative Deep Dexterous Grasping in Clutter</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.04783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483683" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=bKjBVwDBYvg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/aalto-intelligent-robotics/DDGC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Recent advances in multi-fingered robotic grasping have enabled fast 6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in cluttered scenes, on the other hand, remains mostly unexplored due to the added difficulty of reasoning over obstacles which greatly increases the computational time to generate high-quality collision-free grasps. In this work we address such limitations by introducing DDGC, a fast generative multi-finger grasp sampling method that can generate high quality grasps in cluttered scenes from a single RGB-D image. DDGC is built as a network that encodes scene information to produce coarse-to-fine collision-free grasp poses and configurations. We experimentally benchmark DDGC against the simulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and 7 real world scenes. The results show that DDGC outperforms the baseline on synthesizing high-quality grasps and removing clutter while being 5 times faster. This, in turn, opens up the door for using multi-finger grasps in practical applications which has so far been limited due to the excessive computation time needed by other methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202110_lundell_ddgc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{DDGC}: {Generative} {Deep} {Dexterous} {Grasping} in 
                   {Clutter}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{DDGC}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2021.3096239}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6899--6906}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_1_21-480.webp 480w,/assets/img/publication_preview/ral_1_21-800.webp 800w,/assets/img/publication_preview/ral_1_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ral_1_21.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ral_1_21.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202104_nguyen_le_probabilistic" class="col-sm-8"> <div class="title">Probabilistic Surface Friction Estimation Based on Visual and Haptic Measurements</div> <div class="author"> Tran Nguyen Le, <em>Francesco Verdoja</em>, Fares J. Abu-Dakka, and Ville Kyrki </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2010.08277" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=OXXGk5Byu3g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Accurately modeling local surface properties of objects is crucial to many robotic applications, from grasping to material recognition. Surface properties like friction are however difficult to estimate, as visual observation of the object does not convey enough information over these properties. In contrast, haptic exploration is time consuming as it only provides information relevant to the explored parts of the object. In this work, we propose a joint visuo-haptic object model that enables the estimation of surface friction coefficient over an entire object by exploiting the correlation of visual and haptic information, together with a limited haptic exploration by a robotic arm. We demonstrate the validity of the proposed method by showing its ability to estimate varying friction coefficients on a range of real multi-material objects. Furthermore, we illustrate how the estimated friction coefficients can improve grasping success rate by guiding a grasp planner toward high friction areas.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202104_nguyen_le_probabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic {Surface} {Friction} {Estimation} {Based} on 
                   {Visual} and {Haptic} {Measurements}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2021.3062585}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen Le, Tran and Verdoja, Francesco and Abu-Dakka, Fares J. and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2838--2845}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICAR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icar_21-480.webp 480w,/assets/img/publication_preview/icar_21-800.webp 800w,/assets/img/publication_preview/icar_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icar_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icar_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202112_kinnari_gnss-denied" class="col-sm-8"> <div class="title">GNSS-denied geolocalization of UAVs by visual matching of onboard camera images with orthophotos</div> <div class="author"> Jouko Kinnari, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2021 IEEE Int. Conf. on Advanced Robotics (ICAR)</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.14381" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2103.14381" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Localization of low-cost Unmanned Aerial Vehicles (UAVs) often relies on Global Navigation Satellite Systems (GNSS). GNSS are susceptible to both natural disruptions to radio signal and intentional jamming and spoofing by an adversary. A typical way to provide georeferenced localization without GNSS for small UAVs is to have a downward-facing camera and match camera images to a map. The downward-facing camera adds cost, size, and weight to the UAV platform and the orientation limits its usability for other purposes. In this work, we propose a Monte-Carlo localization method for georeferenced localization of an UAV requiring no infrastructure using only inertial measurements, a camera facing an arbitrary direction, and an orthoimage map. We perform orthorectification of the UAV image, relying on a local planarity assumption of the environment, relaxing the requirement of downward-pointing camera. We propose a measure of goodness for the matching score of an orthorectified UAV image and a map. We demonstrate that the system is able to localize globally an UAV with modest requirements for initialization and map resolution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202112_kinnari_gnss-denied</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{GNSS}-denied geolocalization of {UAVs} by visual matching of 
                   onboard camera images with orthophotos}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICAR53236.2021.9659333}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 {IEEE} {Int.} {Conf.} on {Advanced} {Robotics} ({ICAR})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kinnari, Jouko and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{555--562}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECMR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecmr_21-480.webp 480w,/assets/img/publication_preview/ecmr_21-800.webp 800w,/assets/img/publication_preview/ecmr_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ecmr_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecmr_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202108_dengler_online" class="col-sm-8"> <div class="title">Online Object-Oriented Semantic Mapping and Map Updating</div> <div class="author"> Nils Dengler, Tobias Zaenker, <em>Francesco Verdoja</em>, and Maren Bennewitz </div> <div class="periodical"> <em>In 2021 Eur. Conf. on Mobile Robots (ECMR)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2011.06895" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2011.06895" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/NilsDengler/sem_mapping" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Creating and maintaining an accurate representation of the environment is an essential capability for every service robot. Especially for household robots acting in indoor environments, semantic information is important. In this paper, we present a semantic mapping framework with modular map representations. Our system is capable of online mapping and object updating given object detections from RGB-D data and provides various 2D and 3D representations of the mapped objects. To undo wrong data associations, we perform a refinement step when updating object shapes. Furthermore, we maintain an existence likelihood for each object to deal with false positive and false negative detections and keep the map updated. Our mapping system is highly efficient and achieves a run time of more than 10 Hz. We evaluated our approach in various environments using two different robots, i.e., a Toyota HSR and a Fraunhofer Care-O-Bot-4. As the experimental results demonstrate, our system is able to generate maps that are close to the ground truth and outperforms an existing approach in terms of intersection over union, different distance metrics, and the number of correct object mappings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202108_dengler_online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online {Object}-{Oriented} {Semantic} {Mapping} and {Map} 
                   {Updating}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ECMR50962.2021.9568817}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 {Eur.} {Conf.} on {Mobile} {Robots} ({ECMR})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dengler, Nils and Zaenker, Tobias and Verdoja, Francesco and Bennewitz, Maren}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_21-480.webp 480w,/assets/img/publication_preview/icra_21-800.webp 800w,/assets/img/publication_preview/icra_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202105_lundell_multi-fingan" class="col-sm-8"> <div class="title">Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps</div> <div class="author"> Jens Lundell, Enric Corona, Tran Nguyen Le, <em>Francesco Verdoja</em>, Philippe Weinzaepfel, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Grégory Rogez, Francesc Moreno-Noguer, Ville Kyrki' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2021 IEEE Int. Conf. on Robotics and Automation (ICRA)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2012.09696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2012.09696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/Multi-FinGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While there exists many methods for manipulating rigid objects with parallel-jaw grippers, grasping with multi-finger robotic hands remains a quite unexplored research topic. Reasoning and planning collision-free trajectories on the additional degrees of freedom of several fingers represents an important challenge that, so far, involves computationally costly and slow processes. In this work, we present Multi-FinGAN, a fast generative multi-finger grasp sampling method that synthesizes high quality grasps directly from RGB-D images in about a second. We achieve this by training in an end-to-end fashion a coarse-to-fine model composed of a classification network that distinguishes grasp types according to a specific taxonomy and a refinement network that produces refined grasp poses and joint angles. We experimentally validate and benchmark our method against a standard grasp-sampling method on 790 grasps in simulation and 20 grasps on a real Franka Emika Panda. All experimental results using our method show consistent improvements both in terms of grasp quality metrics and grasp success rate. Remarkably, our approach is up to 20-30 times faster than the baseline, a significant improvement that opens the door to feedback-based grasp re-planning and task informative grasping. Code is available at this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202105_lundell_multi-fingan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-{FinGAN}: {Generative} {Coarse}-{To}-{Fine} {Sampling} of 
                   {Multi}-{Finger} {Grasps}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Multi-{FinGAN}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48506.2021.9561228}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                   ({ICRA})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Corona, Enric and Nguyen Le, Tran and Verdoja, Francesco and Weinzaepfel, Philippe and Rogez, Grégory and Moreno-Noguer, Francesc and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4495--4501}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">workshop articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icml_21-480.webp 480w,/assets/img/publication_preview/icml_21-800.webp 800w,/assets/img/publication_preview/icml_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icml_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202107_verdoja_notes" class="col-sm-8"> <div class="title">Notes on the Behavior of MC Dropout</div> <div class="author"> <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> Presented at the “Uncertainty and Robustness in Deep Learning (UDL)” workshop at the 2021 Int. Conf. on Machine Learning (ICML) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2008.02627" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2008.02627" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/mc-dropout-notebooks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Among the various options to estimate uncertainty in deep neural networks, Monte-Carlo dropout is widely popular for its simplicity and effectiveness. However the quality of the uncertainty estimated through this method varies and choices in architecture design and in training procedures have to be carefully considered and tested to obtain satisfactory results. In this paper we present a study offering a different point of view on the behavior of Monte-Carlo dropout, which enables us to observe a few interesting properties of the technique to keep in mind when considering its use for uncertainty estimation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202107_verdoja_notes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Notes on the {Behavior} of {MC} {Dropout}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://sites.google.com/view/udlworkshop2021}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Uncertainty and Robustness in Deep
                   Learning (UDL)'' workshop at the 2021 Int.\ Conf.\ on Machine 
                   Learning (ICML)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">technical reports</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rose_21-480.webp 480w,/assets/img/publication_preview/rose_21-800.webp 800w,/assets/img/publication_preview/rose_21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/rose_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rose_21.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202100_kyrki_robots" class="col-sm-8"> <div class="title">Robots and the Future of Welfare Services: A Finnish Roadmap</div> <div class="author"> Ville Kyrki, Iina Aaltonen, Antti Ainasoja, Päivi Heikkilä, Sari Heikkinen, and <span class="more-authors" title="click to view 31 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '31 more authors' ? 'Lea Hennala, Pertti Koistinen, Joni Kämäräinen, Kalle Laakso, Arto Laitinen, Hanna Lammi, Marinka Lanne, Inka Lappalainen, Hannu Lehtinen, Paula Lehto, Teppo Leppälahti, Jens Lundell, Helinä Melkas, Marketta Niemelä, Satu Parjanen, Jaana Parviainen, Satu Pekkarinen, Jari Pirhonen, Jaakko Porokuokka, Teemu Rantanen, Ismo Ruohomäki, Riika Saurio, Otto Sahlgren, Tuomo Särkikoski, Heli Talja, Antti Tammela, Outi Tuisku, Tuuli Turja, Lina Van Aerschot, Francesco Verdoja, Kari Välimäki' : '31 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">31 more authors</span> </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aaltodoc.aalto.fi/bitstream/123456789/107147/1/isbn9789526403236.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This roadmap summarises a six-year multidisciplinary research project called Robots and the Future of Welfare Services (ROSE), funded by the Strategic Research Council (SRC) established within the Academy of Finland. The objective of the project was to study the current and expected technical opportunities and applications of robotics in welfare services, particularly in care services for older people. The research was carried out at three levels: individual, organisational and societal. The roadmap provides highlights of the various research activities of ROSE. We have studied the perspectives of older adults and care professionals as users of robots, how care organisations are able to adopt and utilise robots in their services, how technology companies find robots as business opportunity, and how the care robotics innovation ecosystem is evolving. Based on these and other studies, we evaluate the development and use of robots in care for older adults in terms of social, ethical-philosophical and political impacts as well as the public discussion on care robots. It appears that there are many single- or limited-purpose robot applications already commercially available in care services for older adults. To be widely adopted, robots should still increase maturity to be able to meet the requirements of care environments, such as in terms of their ability to move in smaller crowded spaces, easy and natural user interaction, and task flexibility. The roadmap provides visions of what could be technically expected in five and ten years. However, at the same time, organisations’ capabilities of adopting new technology and integrating it into services should be supported for them to be able to realise the potential of robots for the benefits of care workers and older persons, as well as the whole society. This roadmap also provides insight into the wider impacts and risks of robotization in society and how to steer it in a responsible way, presented as eight policy recommendations. We also discuss the ROSE project research as a multidisciplinary activity and present lessons learnt.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">202100_kyrki_robots</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robots and the {Future} of {Welfare} {Services}: {A} {Finnish} 
                   {Roadmap}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Robots and the {Future} of {Welfare} {Services}}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">institution</span> <span class="p">=</span> <span class="s">{Aalto University}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kyrki, Ville and Aaltonen, Iina and Ainasoja, Antti and Heikkilä, Päivi and Heikkinen, Sari and Hennala, Lea and Koistinen, Pertti and Kämäräinen, Joni and Laakso, Kalle and Laitinen, Arto and Lammi, Hanna and Lanne, Marinka and Lappalainen, Inka and Lehtinen, Hannu and Lehto, Paula and Leppälahti, Teppo and Lundell, Jens and Melkas, Helinä and Niemelä, Marketta and Parjanen, Satu and Parviainen, Jaana and Pekkarinen, Satu and Pirhonen, Jari and Porokuokka, Jaakko and Rantanen, Teemu and Ruohomäki, Ismo and Saurio, Riika and Sahlgren, Otto and Särkikoski, Tuomo and Talja, Heli and Tammela, Antti and Tuisku, Outi and Turja, Tuuli and Aerschot, Lina Van and Verdoja, Francesco and Välimäki, Kari}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{72}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <h3 class="bibliography">journal articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MVAP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mvap_20-480.webp 480w,/assets/img/publication_preview/mvap_20-800.webp 800w,/assets/img/publication_preview/mvap_20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mvap_20.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mvap_20.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202001_verdoja_graph" class="col-sm-8"> <div class="title">Graph Laplacian for image anomaly detection</div> <div class="author"> <em>Francesco Verdoja</em>, and Marco Grangetto </div> <div class="periodical"> <em>Machine Vision and Applications</em>, Jan 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1802.09843" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/content/pdf/10.1007/s00138-020-01059-4.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/fverdoja/LAD-Laplacian-Anomaly-Detector" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Reed-Xiaoli detector (RXD) is recognized as the benchmark algorithm for image anomaly detection; however, it presents known limitations, namely the dependence over the image following a multivariate Gaussian model, the estimation and inversion of a high-dimensional covariance matrix, and the inability to effectively include spatial awareness in its evaluation. In this work, a novel graph-based solution to the image anomaly detection problem is proposed; leveraging the graph Fourier transform, we are able to overcome some of RXD’s limitations while reducing computational cost at the same time. Tests over both hyperspectral and medical images, using both synthetic and real anomalies, prove the proposed technique is able to obtain significant gains over performance by other algorithms in the state of the art.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202001_verdoja_graph</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Graph {Laplacian} for image anomaly detection}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{31}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s00138-020-01059-4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Machine Vision and Applications}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Grangetto, Marco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_20-480.webp 480w,/assets/img/publication_preview/mfi_20-800.webp 800w,/assets/img/publication_preview/mfi_20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mfi_20.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_20.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202009_zaenker_hypermap" class="col-sm-8"> <div class="title">Hypermap Mapping Framework and its Application to Autonomous Semantic Exploration</div> <div class="author"> Tobias Zaenker, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2020 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.09526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1909.09526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=mdxpBlK4qjk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Eruvae/hypermap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Modern intelligent and autonomous robotic applications often require robots to have more information about their environment than that provided by traditional occupancy grid maps. For example, a robot tasked to perform autonomous semantic exploration has to label objects in the environment it is traversing while autonomously navigating. To solve this task the robot needs to at least maintain an occupancy map of the environment for navigation, an exploration map keeping track of which areas have already been visited, and a semantic map where locations and labels of objects in the environment are recorded. As the number of maps required grows, an application has to know and handle different map representations, which can be a burden.We present the Hypermap framework, which can manage multiple maps of different types. In this work, we explore the capabilities of the framework to handle occupancy grid layers and semantic polygonal layers, but the framework can be extended with new layer types in the future. Additionally, we present an algorithm to automatically generate semantic layers from RGB-D images. We demonstrate the utility of the framework using the example of autonomous exploration for semantic mapping.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202009_zaenker_hypermap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hypermap {Mapping} {Framework} and its {Application} to 
                   {Autonomous} {Semantic} {Exploration}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI49285.2020.9235231}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zaenker, Tobias and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{133--139}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_20-480.webp 480w,/assets/img/publication_preview/icra_20-800.webp 800w,/assets/img/publication_preview/icra_20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_20.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_20.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202005_lundell_beyond" class="col-sm-8"> <div class="title">Beyond Top-Grasps Through Scene Completion</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2020 IEEE Int. Conf. on Robotics and Automation (ICRA)</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.12908" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1909.12908" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=Pret0HNU5us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202005_lundell_beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond {Top}-{Grasps} {Through} {Scene} {Completion}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA40945.2020.9197320}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                   ({ICRA})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{545--551}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">workshop articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_w_20-480.webp 480w,/assets/img/publication_preview/icra_w_20-800.webp 800w,/assets/img/publication_preview/icra_w_20-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icra_w_20.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_w_20.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="202005_verdoja_potential" class="col-sm-8"> <div class="title">On the Potential of Smarter Multi-layer Maps</div> <div class="author"> <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2020 </div> <div class="periodical"> Presented at the “Perception, Action, Learning (PAL)” workshop at the 2020 IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.11094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/pdf/2005.11094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=s6LWzBsc7Ao" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>The most common way for robots to handle environmental information is by using maps. At present, each kind of data is hosted on a separate map, which complicates planning because a robot attempting to perform a task needs to access and process information from many different maps. Also, most often correlation among the information contained in maps obtained from different sources is not evaluated or exploited. In this paper, we argue that in robotics a shift from single-source maps to a multi-layer mapping formalism has the potential to revolutionize the way robots interact with knowledge about their environment. This observation stems from the raise in metric-semantic mapping research, but expands to include in its formulation also layers containing other information sources, e.g., people flow, room semantic, or environment topology. Such multi-layer maps, here named hypermaps, not only can ease processing spatial data information but they can bring added benefits arising from the interaction between maps. We imagine that a new research direction grounded in such multi-layer mapping formalism for robots can use artificial intelligence to process the information it stores to present to the robot task-specific information simplifying planning and bringing us one step closer to high-level reasoning in robots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202005_verdoja_potential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the {Potential} of {Smarter} {Multi}-layer {Maps}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://mit-spark.github.io/PAL-ICRA2020}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Perception, Action, Learning (PAL)'' 
                   workshop at the 2020 IEEE Int.\ Conf.\ on Robotics and 
                   Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_19-480.webp 480w,/assets/img/publication_preview/iros_19-800.webp 800w,/assets/img/publication_preview/iros_19-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_19.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_19.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201911_lundell_robust" class="col-sm-8"> <div class="title">Robust Grasp Planning Over Uncertain Shape Completions</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2019 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Nov 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1903.00645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1903.00645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/robust_grasp_planning_over_uncertain_shape_completions" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a method for planning robust grasps over uncertain shape completed objects. For shape completion, a deep neural network is trained to take a partial view of the object as input and outputs the completed shape as a voxel grid. The key part of the network is dropout layers which are enabled not only during training but also at run-time to generate a set of shape samples representing the shape uncertainty through Monte Carlo sampling. Given the set of shape completed objects, we generate grasp candidates on the mean object shape but evaluate them based on their joint performance in terms of analytical grasp metrics on all the shape candidates. We experimentally validate and benchmark our method against another state-of-the-art method with a Barrett hand on 90000 grasps in simulation and 200 grasps on a real Franka Emika Panda. All experimental results show statistically significant improvements both in terms of grasp quality metrics and grasp success rate, demonstrating that planning shape-uncertainty-aware grasps brings significant advantages over solely planning on a single shape estimate, especially when dealing with complex or unknown objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201911_lundell_robust</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Macau, China}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} 
                   {Completions}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS40897.2019.8967816}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                   {Intelligent} {Robots} and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1526--1532}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Humanoids</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/humanoids_19-480.webp 480w,/assets/img/publication_preview/humanoids_19-800.webp 800w,/assets/img/publication_preview/humanoids_19-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/humanoids_19.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="humanoids_19.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201910_verdoja_deep" class="col-sm-8"> <div class="title">Deep Network Uncertainty Maps for Indoor Navigation</div> <div class="author"> <em>Francesco Verdoja</em>, Jens Lundell, and Ville Kyrki </div> <div class="periodical"> <em>In 2019 IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids)</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1809.04891v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1809.04891v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=J5GOfbPTH88" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/aalto-intelligent-robotics/uncertain_turtlebot_navigation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Most mobile robots for indoor use rely on 2D laser scanners for localization, mapping and navigation. These sensors, however, cannot detect transparent surfaces or measure the full occupancy of complex objects such as tables. Deep Neural Networks have recently been proposed to overcome this limitation by learning to estimate object occupancy. These estimates are nevertheless subject to uncertainty, making the evaluation of their confidence an important issue for these measures to be useful for autonomous navigation and mapping. In this work we approach the problem from two sides. First we discuss uncertainty estimation in deep models, proposing a solution based on a fully convolutional neural network. The proposed architecture is not restricted by the assumption that the uncertainty follows a Gaussian model, as in the case of many popular solutions for deep model uncertainty estimation, such as Monte-Carlo Dropout. We present results showing that uncertainty over obstacle distances is actually better modeled with a Laplace distribution. Then, we propose a novel approach to build maps based on Deep Neural Network uncertainty models. In particular, we present an algorithm to build a map that includes information over obstacle distance estimates while taking into account the level of uncertainty in each estimate. We show how the constructed map can be used to increase global navigation safety by planning trajectories which avoid areas of high uncertainty, enabling higher autonomy for mobile robots in indoor settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201910_verdoja_deep</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep {Network} {Uncertainty} {Maps} for {Indoor} {Navigation}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/Humanoids43949.2019.9035016}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 {IEEE-RAS} {Int.}\ {Conf.}\ on {Humanoid} {Robots} 
                   ({Humanoids})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Lundell, Jens and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112--119}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_18-480.webp 480w,/assets/img/publication_preview/iros_18-800.webp 800w,/assets/img/publication_preview/iros_18-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iros_18.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_18.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201810_lundell_hallucinating" class="col-sm-8"> <div class="title">Hallucinating robots: Inferring obstacle distances from partial laser measurements</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2018 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1805.12338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1805.12338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=J5GOfbPTH88" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/jsll/IROS2018-Hallucinating-Robots" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Many mobile robots rely on 2D laser scanners for localization, mapping, and navigation. However, those sensors are unable to correctly provide distance to obstacles such as glass panels and tables whose actual occupancy is invisible at the height the sensor is measuring. In this work, instead of estimating the distance to obstacles from richer sensor readings such as 3D lasers or RGBD sensors, we present a method to estimate the distance directly from raw 2D laser data. To learn a mapping from raw 2D laser distances to obstacle distances we frame the problem as a learning task and train a neural network formed as an autoencoder. A novel configuration of network hyperparameters is proposed for the task at hand and is quantitatively validated on a test set. Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the trained network can successfully infer obstacle distances from partial 2D laser readings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201810_lundell_hallucinating</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Madrid, Spain}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hallucinating robots: {Inferring} obstacle distances from
                   partial laser measurements}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS.2018.8594399}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2018 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                   {Intelligent} {Robots} and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4781--4787}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">patents</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patent_2_18-480.webp 480w,/assets/img/publication_preview/patent_2_18-800.webp 800w,/assets/img/publication_preview/patent_2_18-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/patent_2_18.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patent_2_18.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201809_grangetto_method" class="col-sm-8"> <div class="title">Method and Apparatus for Encoding and Decoding Digital Images or Video Streams</div> <div class="author"> Marco Grangetto, and <em>Francesco Verdoja</em> </div> <div class="periodical"> Sep 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patentimages.storage.googleapis.com/22/35/dc/d31d103dd9f3dd/US20200014955A1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>A method for encoding digital images or video streams, includes a receiving phase, wherein a portion of an image is received; a graph weights prediction phase, wherein the elements of a weights matrix associated to the graph related to the blocks of the image (predicted blocks) are predicted on the basis of reconstructed, de-quantized and inverse-transformed pixel values of at least one previously coded block (predictor block) of the image, the weights matrix being a matrix comprising elements denoting the level of similarity between a pair of pixels composing said image, a graph transform computation phase, wherein the graph Fourier transform of the blocks of the image is performed, obtaining for the blocks a set of coefficients determined on the basis of the predicted weights; a coefficients quantization phase, wherein the coefficients are quantized an output phase wherein a bitstream comprising the transformed and quantized coefficients is transmitted and/or stored.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">201809_grangetto_method</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Method and {Apparatus} for {Encoding} and {Decoding} {Digital} 
                   {Images} or {Video} {Streams}}</span><span class="p">,</span>
  <span class="na">assignee</span> <span class="p">=</span> <span class="s">{Sisvel Technology S.r.l}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{WO2018158735 (A1)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grangetto, Marco and Verdoja, Francesco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patent_1_18-480.webp 480w,/assets/img/publication_preview/patent_1_18-800.webp 800w,/assets/img/publication_preview/patent_1_18-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/patent_1_18.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patent_1_18.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201809_grangetto_methods" class="col-sm-8"> <div class="title">Methods and Apparatuses for Encoding and Decoding Superpixel Borders</div> <div class="author"> Marco Grangetto, and <em>Francesco Verdoja</em> </div> <div class="periodical"> Sep 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patentimages.storage.googleapis.com/f2/f7/12/50250a322ba55b/US10708601.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The present invention relates to a method for encoding the borders of pixel regions of an image, wherein the borders contain a sequence of vertices subdividing the image into regions of pixels (superpixels), by generating a sequence of symbols from an alphabet including the step of: defining for each superpixel a first vertex for coding the borders of the superpixel according to a criterion common to all superpixels; defining for each superpixel the same coding order of the border vertices, either clockwise or counter-clockwise; defining the order for coding the superpixels on the base of a common rule depending on the relative positions of the first vertices; defining a set of vertices as a known border, wherein the following steps are performed for selecting a symbol of the alphabet, for encoding the borders of the superpixels: a) determining the first vertex of the next superpixel border individuated by the common criterion; b) determining the next vertex to be encoded on the basis of the coding direction; c) selecting a first symbol (“0”) for encoding the next vertex if the next vertex of a border pertains to the known border, d) selecting a symbol (“1”; “2”) different from the first symbol (“0”) if the next vertex is not in the known border; e) repeating steps b), c), d) and e) until all vertices of the superpixel border have been encoded; f) adding each vertex of the superpixel border that was not in the known border to the set; g) determining the next superpixel whose border is to be encoded according to the common rule, if any; i) repeating steps a)-g) until the borders of all the superpixels of the image have being added to the known border.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">201809_grangetto_methods</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Methods and {Apparatuses} for {Encoding} and {Decoding} 
                   {Superpixel} {Borders}}</span><span class="p">,</span>
  <span class="na">assignee</span> <span class="p">=</span> <span class="s">{Sisvel Technology S.r.l}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{WO2018158738 (A1)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grangetto, Marco and Verdoja, Francesco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <h3 class="bibliography">theses</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thesis_17-480.webp 480w,/assets/img/publication_preview/thesis_17-800.webp 800w,/assets/img/publication_preview/thesis_17-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/thesis_17.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thesis_17.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201707_verdoja_use" class="col-sm-8"> <div class="title">The use of Graph Fourier Transform in image processing: a new solution to classical problems</div> <div class="author"> <em>Francesco Verdoja</em> </div> <div class="periodical"> Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iris.unito.it/retrieve/handle/2318/1645345/352523/PhdThesis_Verdoja.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Graph-based approaches have recently seen a spike of interest in the image processing and computer vision communities, and many classical problems are finding new solutions thanks to these techniques. The Graph Fourier Transform (GFT), the equivalent of the Fourier transform for graph signals, is used in many domains to analyze and process data modeled by a graph. In this thesis we present some classical image processing problems that can be solved through the use of GFT. We’ll focus our attention on two main research area: the first is image compression, where the use of the GFT is finding its way in recent literature; we’ll propose two novel ways to deal with the problem of graph weight encoding. We’ll also propose approaches to reduce overhead costs of shape-adaptive compression methods. The second research field is image anomaly detection, GFT has never been proposed to this date to solve this class of problems; we’ll discuss here a novel technique and we’ll test its application on hyperspectral and medical images. We’ll show how graph approaches can be used to generalize and improve performance of the widely popular RX Detector, by reducing its computational complexity while at the same time fixing the well known problem of its dependency from covariance matrix estimation and inversion. All our experiments confirm that graph-based approaches leveraging on the GFT can be a viable option to solve tasks in multiple image processing domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">201707_verdoja_use</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Torino, Italy}</span><span class="p">,</span>
  <span class="na">type</span> <span class="p">=</span> <span class="s">{Doctoral dissertation}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The use of {Graph} {Fourier} {Transform} in image processing: a 
                   new solution to classical problems}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{The use of {Graph} {Fourier} {Transform} in image processing}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Università degli Studi di Torino}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICME</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icme_17-480.webp 480w,/assets/img/publication_preview/icme_17-800.webp 800w,/assets/img/publication_preview/icme_17-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icme_17.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icme_17.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201707_verdoja_fast" class="col-sm-8"> <div class="title">Fast 3D point cloud segmentation using supervoxels with geometry and color for 3D scene understanding</div> <div class="author"> <em>Francesco Verdoja</em>, Diego Thomas, and Akihiro Sugimoto </div> <div class="periodical"> <em>In 2017 IEEE Int. Conf. on Multimedia and Expo (ICME)</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/fverdoja/Fast-3D-Pointcloud-Segmentation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Segmentation of 3D colored point clouds is a research field with renewed interest thanks to recent availability of inexpensive consumer RGB-D cameras and its importance as an unavoidable low-level step in many robotic applications. However, 3D data’s nature makes the task challenging and, thus, many different techniques are being proposed , all of which require expensive computational costs. This paper presents a novel fast method for 3D colored point cloud segmen-tation. It starts with supervoxel partitioning of the cloud, i.e., an oversegmentation of the points in the cloud. Then it leverages on a novel metric exploiting both geometry and color to iteratively merge the supervoxels to obtain a 3D segmentation where the hierarchical structure of partitions is maintained. The algorithm also presents computational complexity linear to the size of the input. Experimental results over two publicly available datasets demonstrate that our proposed method outperforms state-of-the-art techniques.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201707_verdoja_fast</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Hong Kong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast 3D point cloud segmentation using supervoxels with
                   geometry and color for 3D scene understanding}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICME.2017.8019382}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 {IEEE} {Int.}\ {Conf.}\ on {Multimedia} and 
                   {Expo} ({ICME})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Thomas, Diego and Sugimoto, Akihiro}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1285--1290}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icassp_2_17-480.webp 480w,/assets/img/publication_preview/icassp_2_17-800.webp 800w,/assets/img/publication_preview/icassp_2_17-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icassp_2_17.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icassp_2_17.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201703_verdoja_directional" class="col-sm-8"> <div class="title">Directional graph weight prediction for image compression</div> <div class="author"> <em>Francesco Verdoja</em>, and Marco Grangetto </div> <div class="periodical"> <em>In 2017 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Graph-based models have recently attracted attention for their potential to enhance transform coding image compression thanks to their capability to efficiently represent discontinuities. Graph transform gets closer to the optimal KLT by using weights that represent inter-pixel correlations but the extra cost to provide such weights can overwhelm the gain, especially in the case of natural images rich of details. In this paper we provide a novel idea to make graph transform adaptive to the actual image content, avoiding the need to encode the graph weights as side information. We show that an approach similar to spatial prediction can be used to effectively predict graph weights in place of pixels; in particular, we propose the design of directional graph weight prediction modes and show the resulting coding gain. The proposed approach can be used jointly with other graph based intra prediction methods to further enhance compression. Our comparative experimental analysis, carried out with a fully fledged still image coding prototype, shows that we are able to achieve significant coding gains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201703_verdoja_directional</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Orleans, LA}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Directional graph weight prediction for image compression}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP.2017.7952410}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 {IEEE} {Int.}\ {Conf.}\ on {Acoustics}, 
                   {Speech} and {Signal} {Processing} ({ICASSP})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Grangetto, Marco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1517--1521}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icassp_1_17-480.webp 480w,/assets/img/publication_preview/icassp_1_17-800.webp 800w,/assets/img/publication_preview/icassp_1_17-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icassp_1_17.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icassp_1_17.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201703_verdoja_efficient" class="col-sm-8"> <div class="title">Efficient representation of segmentation contours using chain codes</div> <div class="author"> <em>Francesco Verdoja</em>, and Marco Grangetto </div> <div class="periodical"> <em>In 2017 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952399" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Segmentation is one of the most important low-level tasks in image processing as it enables many higher level computer vision tasks like object recognition and tracking. Segmentation can also be exploited for image compression using recent graph-based algorithms, provided that the corresponding contours can be represented efficiently. Transmission of borders is also key to distributed computer vision. In this paper we propose a new chain code tailored to compress segmentation contours. Based on the widely known 3OT, our algorithm is able to encode regions avoiding borders it has already coded once and without the need of any starting point information for each region. We tested our method against three other state of the art chain codes over the BSDS500 dataset, and we demonstrated that the proposed chain code achieves the highest compression ratio, resulting on average in over 27% bit-per-pixel saving.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201703_verdoja_efficient</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Orleans, LA}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient representation of segmentation contours using chain 
                   codes}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP.2017.7952399}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 {IEEE} {Int.}\ {Conf.}\ on {Acoustics}, 
                   {Speech} and {Signal} {Processing} ({ICASSP})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Grangetto, Marco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1462--1466}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">patents</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patent_17-480.webp 480w,/assets/img/publication_preview/patent_17-800.webp 800w,/assets/img/publication_preview/patent_17-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/patent_17.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="patent_17.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201703_fracastoro_methods" class="col-sm-8"> <div class="title">Methods and Apparatuses for Encoding and Decoding Digital Images Through Superpixels</div> <div class="author"> Giulia Fracastoro, Enrico Magli, <em>Francesco Verdoja</em>, and Marco Grangetto </div> <div class="periodical"> Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://patentimages.storage.googleapis.com/f4/8a/16/bab6bd298dea8d/US20180278957A1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>A method and an apparatus for encoding and/or decoding digital images or video streams are provided, wherein the encoding apparatus includes a processor configured for reading at least a portion of the image, segmenting the portion of the image in order to obtain groups of pixels identified by borders information and containing at least two pixels having one or more homogeneous characteristics, computing, for each group of pixels, a weight map on the basis of the borders information associated to the group of pixels, a graph transform matrix on the basis of the weight map, and transform coefficients on the basis of the graph transform matrix (U) and of the pixels contained in the group of pixels.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@patent</span><span class="p">{</span><span class="nl">201703_fracastoro_methods</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Methods and {Apparatuses} for {Encoding} and {Decoding} 
                   {Digital} {Images} {Through} {Superpixels}}</span><span class="p">,</span>
  <span class="na">assignee</span> <span class="p">=</span> <span class="s">{Sisvel Technology Srl; Politecnico Di Torino}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{WO2017051358 (A1)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fracastoro, Giulia and Magli, Enrico and Verdoja, Francesco and Grangetto, Marco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <h3 class="bibliography">book chapters</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AIFM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/aifm_16-480.webp 480w,/assets/img/publication_preview/aifm_16-800.webp 800w,/assets/img/publication_preview/aifm_16-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/aifm_16.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aifm_16.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201602_bracco_automatic" class="col-sm-8"> <div class="title">Automatic GTV contouring applying anomaly detection algorithm on dynamic FDG PET images</div> <div class="author"> Christian Bracco, <em>Francesco Verdoja</em>, Marco Grangetto, Amalia Di Dia, Manuela Racca, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Teresio Varetto, Michele Stasi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">2 more authors</span> </div> <div class="periodical"> <em>In Abstracts of the 9th National Congress of the Associazione Italiana di Fisica Medica</em>, Feb 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iris.unito.it/retrieve/e27ce428-8b03-2581-e053-d805fe0acbaa/copertina.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Introduction: The aim of this work is to show the results of GTV automatic segmentation based on dynamic PET acquisition. With respect to single voxel segmentation the temporal information is used to improve quality of GTV delineation. The segmentation algorithm proposed exploits the theoretic assumption that FDG uptake over time in cancer cells is very different from the one in normal tissues and therefore in this study anomaly detection is used to look for tumor peculiar-anomalous TACs. Material and Methods: For each patient two list mode datasets of images were acquired. The first one scan (basal) was acquired one hour after FDG injection and reconstructed as static frame. The last one (delayed) was acquired half one hour after the first scan and reconstructed as dynamic scan. Two delayed scans were registered to the basal scan. A modified version of the RX Detector was used. RX Detector usually works in RGB, but in this study its use on TACs has been explored passing the three grayscale images in place of the three channels of RGB. The resulting single image, which actually is a matrix of Mahalanobis distances, presents values that are very high for voxels whose TAC has anomalous temporal behavior. Finally, threshold segmentation is performed on the distance matrix. On a dataset of 10 patients segmentation techniques present in the literature working on single PET scan have been implemented as well as segmentation techniques based on RX Detector output. Results: Spatial overlap index (SOI) was used as metric to evaluate the segmentation accuracy. All of the segmentation algorithms implemented on RXD output show better SOI (0.507 ± 0.158) than algorithm based on SUV, i.e. Brambilla, SOI 0.278 ± 0.236. A manual contour drawn by experienced Nuclear Physician was the reference. Conclusion: Although a small dataset, the segmentation of dynamic PET images based on RXD output seems to be promising.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">201602_bracco_automatic</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Physica Medica}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic {GTV} contouring applying anomaly detection algorithm 
                   on dynamic {FDG} {PET} images}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.ejmp.2016.01.343}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Abstracts of the 9th National Congress of the Associazione 
                   Italiana di Fisica Medica}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bracco, Christian and Verdoja, Francesco and Grangetto, Marco and Di Dia, Amalia and Racca, Manuela and Varetto, Teresio and Stasi, Michele}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{99}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icip_16-480.webp 480w,/assets/img/publication_preview/icip_16-800.webp 800w,/assets/img/publication_preview/icip_16-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icip_16.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icip_16.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201609_verdoja_global" class="col-sm-8"> <div class="title">Global and local anomaly detectors for tumor segmentation in dynamic PET acquisitions</div> <div class="author"> <em>Francesco Verdoja</em>, Barbara Bonafè, Davide Cavagnino, Marco Grangetto, Christian Bracco, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Teresio Varetto, Manuela Racca, Michele Stasi' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2016 IEEE Int. Conf. on Image Processing (ICIP)</em>, Sep 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://web.archive.org/web/20180718235804id_/https://iris.unito.it/retrieve/handle/2318/1610112/258226/ArticleTumors2016_IRIS.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper we explore the application of anomaly detection techniques to tumor voxels segmentation. The developed algorithms work on 3-points dynamic FDG-PET acquisitions and leverage on the peculiar anaerobic metabolism that cancer cells experience over time. A few different global or local anomaly detectors are discussed, together with an investigation over two different algorithms aiming to estimate normal tissues’ statistical distribution. Finally, all the proposed algorithms are tested on a dataset composed of 9 patients proving that anomaly detectors are able to outperform techniques in the state of the art.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201609_verdoja_global</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Phoenix, AZ}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global and local anomaly detectors for tumor segmentation in 
                   dynamic {PET} acquisitions}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2016.7533137}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 {IEEE} {Int.}\ {Conf.}\ on {Image} 
                   {Processing} ({ICIP})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Bonafè, Barbara and Cavagnino, Davide and Grangetto, Marco and Bracco, Christian and Varetto, Teresio and Racca, Manuela and Stasi, Michele}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4131--4135}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <h3 class="bibliography">book chapters</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIAP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iciap_15-480.webp 480w,/assets/img/publication_preview/iciap_15-800.webp 800w,/assets/img/publication_preview/iciap_15-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/iciap_15.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iciap_15.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201509_verdoja_fast" class="col-sm-8"> <div class="title">Fast Superpixel-Based Hierarchical Approach to Image Segmentation</div> <div class="author"> <em>Francesco Verdoja</em>, and Marco Grangetto </div> <div class="periodical"> <em>In Image Analysis and Processing—ICIAP 2015</em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-23231-7_33.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Image segmentation is one of the core task in image processing. Traditionally such operation is performed starting from single pixels requiring a significant amount of computations. Only recently it has been shown that superpixels can be used to improve segmentation performance. In this work we propose a novel superpixel-based hierarchical approach for image segmentation that works by iteratively merging nodes of a weighted undirected graph initialized with the superpixels regions. Proper metrics to drive the regions merging are proposed and experimentally validated using the standard Berkeley Dataset. Our analysis shows that the proposed algorithm runs faster than state of the art techniques while providing accurate segmentation results both in terms of visual and objective metrics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">201509_verdoja_fast</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture {Notes} in {Computer} {Science}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast {Superpixel}-{Based} {Hierarchical} {Approach} to {Image} 
                   {Segmentation}}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{9279}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-319-23231-7_33}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Image {Analysis} and {Processing}---{ICIAP} 2015}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Grangetto, Marco}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{364--374}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icip_15-480.webp 480w,/assets/img/publication_preview/icip_15-800.webp 800w,/assets/img/publication_preview/icip_15-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icip_15.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icip_15.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201509_fracastoro_superpixel-driven" class="col-sm-8"> <div class="title">Superpixel-driven Graph Transform for Image Compression</div> <div class="author"> Giulia Fracastoro, <em>Francesco Verdoja</em>, Marco Grangetto, and Enrico Magli </div> <div class="periodical"> <em>In 2015 IEEE Int. Conf. on Image Processing (ICIP)</em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best 10% Paper</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iris.unito.it/bitstream/2318/1557609/2/articleICIP15.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Awarded at the 2015 IEEE Int. Conf. on Image Processing (ICIP)</p> </div> <div class="abstract hidden"> <p>Block-based compression tends to be inefficient when blocks contain arbitrary shaped discontinuities. Recently, graph-based approaches have been proposed to address this issue, but the cost of transmitting graph topology often overcome the gain of such techniques. In this work we propose a new Superpixel-driven Graph Transform (SDGT) that uses clusters of superpixels, which have the ability to adhere nicely to edges in the image, as coding blocks and computes inside these homogeneously colored regions a graph transform which is shape-adaptive. Doing so, only the borders of the regions and the transform coefficients need to be transmitted, in place of all the structure of the graph. The proposed method is finally compared to DCT and the experimental results show how it is able to outperform DCT both visually and in term of PSNR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201509_fracastoro_superpixel-driven</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Quebec City, Canada}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Superpixel-driven {Graph} {Transform} for {Image} 
                   {Compression}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2015.7351279}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 {IEEE} {Int.}\ {Conf.}\ on {Image} {Processing} ({ICIP})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fracastoro, Giulia and Verdoja, Francesco and Grangetto, Marco and Magli, Enrico}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2631--2635}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <h3 class="bibliography">conference articles</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icip_14-480.webp 480w,/assets/img/publication_preview/icip_14-800.webp 800w,/assets/img/publication_preview/icip_14-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/icip_14.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icip_14.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="201410_verdoja_automatic" class="col-sm-8"> <div class="title">Automatic method for tumor segmentation from 3-points dynamic PET acquisitions</div> <div class="author"> <em>Francesco Verdoja</em>, Marco Grangetto, Christian Bracco, Teresio Varetto, Manuela Racca, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Michele Stasi' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">1 more author</span> </div> <div class="periodical"> <em>In 2014 IEEE Int. Conf. on Image Processing (ICIP)</em>, Oct 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Student Award</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iris.unito.it/retrieve/handle/2318/154166/148728/PID3258293_4aperto.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Awarded for this work by <em>Microsoft Research Cambridge</em> at the 2014 International Computer Vision Summer School (ICVSS)</p> </div> <div class="abstract hidden"> <p>In this paper a novel technique to segment tumor voxels in dynamic positron emission tomography (PET) scans is proposed. An innovative anomaly detection tool tailored for 3-points dynamic PET scans is designed. The algorithm allows the identification of tumoral cells in dynamic FDG-PET scans thanks to their peculiar anaerobic metabolism experienced over time. The proposed tool is preliminarily tested on a small dataset showing promising performance as compared to the state of the art in terms of both accuracy and classification errors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201410_verdoja_automatic</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Paris, France}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic method for tumor segmentation from 3-points dynamic 
                   {PET} acquisitions}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICIP.2014.7025188}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2014 {IEEE} {Int.}\ {Conf.}\ on {Image} 
                   {Processing} ({ICIP})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Grangetto, Marco and Bracco, Christian and Varetto, Teresio and Racca, Manuela and Stasi, Michele}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{937--941}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Francesco Verdoja. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>