<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SANTTU | francesco verdoja </title> <meta name="author" content="Francesco Verdoja"> <meta name="description" content="2-year co-innovation project on the development of semi-autonomous assistance systems for heavy industrial machines "> <meta name="keywords" content="robotics, mapping, computer vision, artificial intelligence, ai, machine learning, research, academia"> <meta property="og:site_name" content="francesco verdoja"> <meta property="og:type" content="website"> <meta property="og:title" content="francesco verdoja | SANTTU"> <meta property="og:url" content="https://fverdoja.github.io/funding/2022_santtu/"> <meta property="og:description" content="2-year co-innovation project on the development of semi-autonomous assistance systems for heavy industrial machines "> <meta property="og:image" content="assets/img/fra.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="SANTTU"> <meta name="twitter:description" content="2-year co-innovation project on the development of semi-autonomous assistance systems for heavy industrial machines "> <meta name="twitter:image" content="assets/img/fra.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Francesco Verdoja"
        },
        "url": "https://fverdoja.github.io/funding/2022_santtu/",
        "@type": "WebSite",
        "description": "2-year co-innovation project on the development of semi-autonomous assistance systems for heavy industrial machines
",
        "headline": "SANTTU",
        
        "sameAs": ["https://orcid.org/0000-0002-9551-6186", "https://scholar.google.com/citations?user=3DDM3_kAAAAJ", "https://www.scopus.com/authid/detail.uri?authorId=56685641600", "https://github.com/fverdoja", "https://www.linkedin.com/in/fverdoja", "https://research.aalto.fi/en/persons/francesco-verdoja"],
        
        "name": "Francesco Verdoja",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fverdoja.github.io/funding/2022_santtu/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">francesco</span> verdoja </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/events/">events </a> </li> <li class="nav-item active"> <a class="nav-link" href="/funding/">funding <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"><a class="nav-link" href="/assets/pdf/verdoja_francesco.pdf">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">SANTTU</span> | To reduce stress from machine &amp; operator </h1> <p class="post-description">2-year co-innovation project on the development of semi-autonomous assistance systems for heavy industrial machines </p> <ul class="post-tags fa-ul"> <li> <span class="fa-li"><i class="fa-solid fa-landmark fa-sm"></i></span><strong>Funded by:</strong> <a href="https://www.businessfinland.fi/en/for-finnish-customers/services/funding" rel="external nofollow noopener" target="_blank">Business Finland</a> </li> <li> <span class="fa-li"><i class="fa-solid fa-users fa-sm"></i></span><strong>Partners:</strong> Aalto University (PI: Prof. Ville Kyrki), LUT-University, Oulu University, GIM Oy, Sandvik Oy, Ponsse Oy, Raute Oy, Mantsinen Oy </li> <li> <span class="fa-li"><i class="fa-solid fa-user fa-sm"></i></span><strong>Role:</strong> Project manager (Aalto), considerable involvement in proposal preparation, writing, and coordination </li> <li> <span class="fa-li"><i class="fa-solid fa-coins fa-sm"></i></span><strong>Budget:</strong> 2124k€ total, 671k€ Aalto </li> <li> <span class="fa-li"><i class="fa-solid fa-calendar fa-sm"></i></span><strong>Period:</strong> May 2022–Apr 2024 </li> </ul> </header> <article> <h2 id="abstract">abstract</h2> <p>The SANTTU project focuses on the development of systems for operators of work machines and heavy industrial machines. The aim is to simplify the control and operation of the machines with semi-autonomous operator assistance systems. This can be used to reduce the cognitive stress on the operator as well as the stress on the machine, thus extending machine life, availability, and productivity. By combining physics-based (real-time) simulation, digital twins and artificial intelligence technologies - semi-automated systems can be created. We develop a model-based, AI-assisted solution that provides collision prevention, stress reduction, improved accuracy, automation of work routines, and a human-centric user interface design. These lower the competence requirements for the efficient use of the machine, which expands the realistically targeted market, especially in fast-growing market areas.</p> </article> <h2 id="related-publications">related publications</h2> <div class="publications"> <h2 class="bibliography">journal articles</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IJRR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijrr_23-480.webp 480w,/assets/img/publication_preview/ijrr_23-800.webp 800w,/assets/img/publication_preview/ijrr_23-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ijrr_23.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijrr_23.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202309_kucner_survey" class="col-sm-8"> <div class="title">Survey of maps of dynamics for mobile robots</div> <div class="author"> Tomasz Piotr Kucner, Martin Magnusson, Sariah Mghames, Luigi Palmieri, <em>Francesco Verdoja</em>, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Chittaranjan Srinivas Swaminathan, Tomáš Krajník, Erik Schaffernicht, Nicola Bellotto, Marc Hanheide, Achim J Lilienthal' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">6 more authors</span> </div> <div class="periodical"> <em>The Int. Journal of Robotics Research</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://journals.sagepub.com/doi/epub/10.1177/02783649231190428" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robotic mapping provides spatial information for autonomous agents. Depending on the tasks they seek to enable, the maps created range from simple 2D representations of the environment geometry to complex, multilayered semantic maps. This survey article is about maps of dynamics (MoDs), which store semantic information about typical motion patterns in a given environment. Some MoDs use trajectories as input, and some can be built from short, disconnected observations of motion. Robots can use MoDs, for example, for global motion planning, improved localization, or human motion prediction. Accounting for the increasing importance of maps of dynamics, we present a comprehensive survey that organizes the knowledge accumulated in the field and identifies promising directions for future work. Specifically, we introduce field-specific vocabulary, summarize existing work according to a novel taxonomy, and describe possible applications and open research problems. We conclude that the field is mature enough, and we expect that maps of dynamics will be increasingly used to improve robot performance in real-world use cases. At the same time, the field is still in a phase of rapid development where novel contributions could significantly impact this research area.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202309_kucner_survey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Survey of maps of dynamics for mobile robots}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1177/02783649231190428}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Int.\ Journal of Robotics Research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kucner, Tomasz Piotr and Magnusson, Martin and Mghames, Sariah and Palmieri, Luigi and Verdoja, Francesco and Swaminathan, Chittaranjan Srinivas and Krajník, Tomáš and Schaffernicht, Erik and Bellotto, Nicola and Hanheide, Marc and Lilienthal, Achim J}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{977--1006}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">conference articles</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_2_24-480.webp 480w,/assets/img/publication_preview/iros_2_24-800.webp 800w,/assets/img/publication_preview/iros_2_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/iros_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_2_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202410_chaubey_jointly" class="col-sm-8"> <div class="title">Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation</div> <div class="author"> Shivam Chaubey, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best SSRR Paper finalist</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.03491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2405.03491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://version.aalto.fi/gitlab/chaubes1/jointly-learning-cost-and-constraints/-/raw/main/images/video.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://version.aalto.fi/gitlab/chaubes1/jointly-learning-cost-and-constraints" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Nominated for the <em>Best Safety, Security, and Rescue Robotics Paper</em> award sponsored by the International Rescue System Initiative (IRSI) at the 2024 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</p> </div> <div class="abstract hidden"> <p>Learning from Demonstration allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202410_chaubey_jointly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Jointly Learning Cost and Constraints from Demonstrations for
                   Safe Trajectory Generation}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS58592.2024.10802533}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on {Intelligent} {Robots}
                   and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3635--3642}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_2_24-480.webp 480w,/assets/img/publication_preview/mfi_2_24-800.webp 800w,/assets/img/publication_preview/mfi_2_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mfi_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_2_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202409_pekkanen_localization" class="col-sm-8"> <div class="title">Localization Under Consistent Assumptions Over Dynamics</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.16702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2305.16702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Accurate maps are a prerequisite for virtually all mobile robot tasks. Most state-of-the-art maps assume a static world; therefore, dynamic objects are filtered out of the measurements. However, this division ignores movable but non- moving—i.e., semi-static—objects, which are usually recorded in the map and treated as static objects, violating the static world assumption and causing errors in the localization. This paper presents a method for consistently modeling moving and movable objects to match the map and measurements. This reduces the error resulting from inconsistent categorization and treatment of non-static measurements. A semantic segmentation network is used to categorize the measurements into static and semi-static classes, and a background subtraction filter is used to remove dynamic measurements. Finally, we show that consis- tent assumptions over dynamics improve localization accuracy when compared against a state-of-the-art baseline solution using real-world data from the Oxford Radar RobotCar data set.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202409_pekkanen_localization</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Localization {Under} {Consistent} {Assumptions} {Over}
                   {Dynamics}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI62651.2024.10705760}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_1_24-480.webp 480w,/assets/img/publication_preview/mfi_1_24-800.webp 800w,/assets/img/publication_preview/mfi_1_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mfi_1_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_1_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202409_pekkanen_object" class="col-sm-8"> <div class="title">Object-Oriented Grid Mapping in Dynamic Environments</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2024 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2309.08324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/lamide" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Grid maps, especially occupancy grid maps, are ubiquitous in many mobile robot applications. To simplify the process of learning the map, grid maps subdivide the world into a grid of cells whose occupancies are independently estimated using measurements in the perceptual field of the particular cell. However, the world consists of objects that span multiple cells, which means that measurements falling onto a cell provide evidence of the occupancy of other cells belonging to the same object. Current models do not capture this correlation and, therefore, do not use object-level information for estimating the state of the environment. In this work, we present a way to generalize the update of grid maps, relaxing the assumption of independence. We propose modeling the relationship between the measurements and the occupancy of each cell as a set of latent variables and jointly estimate those variables and the posterior of the map. We propose a method to estimate the latent variables by clustering based on semantic labels and an extension to the Normal Distributions Transform Occupancy Map (NDT-OM) to facilitate the proposed map update method. We perform comprehensive map creation and localization ex- periments with real-world data sets and show that the proposed method creates better maps in highly dynamic environments compared to state-of-the-art methods. Finally, we demonstrate the ability of the proposed method to remove occluded objects from the map in a lifelong map update scenario.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202409_pekkanen_object</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Object-{Oriented} {Grid} {Mapping} in {Dynamic}
                   {Environments}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI62651.2024.10705762}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">workshop articles</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_1_24-480.webp 480w,/assets/img/publication_preview/icra_1_24-800.webp 800w,/assets/img/publication_preview/icra_1_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_1_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_1_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202405_chaubey_jointly" class="col-sm-8"> <div class="title">Jointly Learning Cost and Constraints from Demonstrations for Safe Trajectory Generation</div> <div class="author"> Shivam Chaubey, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Towards Collaborative Partners: Design, Shared Control, and Robot Learning for Physical Human-Robot Interaction (pHRI)” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://drive.google.com/file/d/1DZcfnhbppEWc_fM9rWBgJyVI65q5ythz/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Learning from Demonstration (LfD) allows robots to mimic human actions. However, these methods do not model constraints crucial to ensure safety of the learned skill. Moreover, even when explicitly modelling constraints, they rely on the assumption of a known cost function, which limits their practical usability for task with unknown cost. In this work we propose a two-step optimization process that allow to estimate cost and constraints by decoupling the learning of cost functions from the identification of unknown constraints within the demonstrated trajectories. Initially, we identify the cost function by isolating the effect of constraints on parts of the demonstrations. Subsequently, a constraint leaning method is used to identify the unknown constraints. Our approach is validated both on simulated trajectories and a real robotic manipulation task. Our experiments show the impact that incorrect cost estimation has on the learned constraints and illustrate how the proposed method is able to infer unknown constraints, such as obstacles, from demonstrated trajectories without any initial knowledge of the cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_chaubey_jointly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Jointly Learning Cost and Constraints from Demonstrations for
                   Safe Trajectory Generation}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://sites.google.com/view/icra24-physical-hri}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chaubey, Shivam and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Towards Collaborative Partners: Design,
                   Shared Control, and Robot Learning for Physical Human-Robot
                   Interaction (pHRI)'' workshop at the IEEE Int.\ Conf.\ on
                   Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_2_24-480.webp 480w,/assets/img/publication_preview/icra_2_24-800.webp 800w,/assets/img/publication_preview/icra_2_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_2_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_2_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202405_pekkanen_evaluating" class="col-sm-8"> <div class="title">Evaluating the quality of robotic visual-language maps</div> <div class="author"> Matti Pekkanen, Tsvetomila Mihaylova, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Vision-Language Models for Navigation and Manipulation (VLMNM)” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=FspmwJHZ2G" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. In this paper, we propose a way to analyze the quality of maps created using VLMs by evaluating two critical properties: queryability and consistency. We demonstrate the proposed method by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_pekkanen_evaluating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluating the quality of robotic visual-language maps}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://vlmnm-workshop.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Mihaylova, Tsvetomila and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Vision-Language Models for Navigation and
                   Manipulation (VLMNM)'' workshop at the IEEE Int.\ Conf.\ on
                   Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_3_24-480.webp 480w,/assets/img/publication_preview/icra_3_24-800.webp 800w,/assets/img/publication_preview/icra_3_24-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_3_24.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_3_24.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202405_pekkanen_modeling" class="col-sm-8"> <div class="title">Modeling movable objects improves localization in dynamic environments</div> <div class="author"> Matti Pekkanen, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2024 </div> <div class="periodical"> Presented at the “Future of Construction: Lifelong Learning Robots in Changing Construction Sites” workshop at the IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://construction-robots.github.io/papers/49.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most state-of-the-art robotic maps assume a static world; therefore, dynamic objects are filtered out of the measurements. However, this division ignores movable but non- moving, i.e., semi-static objects, which are usually recorded in the map and treated as static objects, violating the static world assumption and causing errors in the localization. This paper presents a method for modeling moving and movable objects to match the map and measurements consistently. This reduces the error resulting from inconsistent categorization and treatment of non-static measurements. A semantic segmentation network is used to categorize the measurements into static and semi- static classes, and a background subtraction-based filtering method is used to remove dynamic measurements. Experimental comparison against a state-of-the-art baseline solution using real-world data from the Oxford Radar RobotCar data set shows that consistent assumptions over dynamics increase localization accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202405_pekkanen_modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling movable objects improves localization in dynamic
                   environments}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://construction-robots.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pekkanen, Matti and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Future of Construction: Lifelong Learning
                   Robots in Changing Construction Sites'' workshop at the IEEE
                   Int.\ Conf.\ on Robotics and Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_22-480.webp 480w,/assets/img/publication_preview/iros_22-800.webp 800w,/assets/img/publication_preview/iros_22-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/iros_22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_22.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202210_verdoja_generating" class="col-sm-8"> <div class="title">Generating people flow from architecture of real unseen environments</div> <div class="author"> <em>Francesco Verdoja</em>, Tomasz Piotr Kucner, and Ville Kyrki </div> <div class="periodical"> Oct 2022 </div> <div class="periodical"> Presented at the “Perception and Navigation for Autonomous Robotics in Unstructured and Dynamic Environments (PNARUDE)” workshop at the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iros2022-pnarude.github.io/pdf/PNARUDE_IROS2022_Francesco_Verdoja_Generating_people_flow_from_architecture_of_real_unseen_environments.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in multi-fingered robotic grasping have enabled fast 6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in cluttered scenes, on the other hand, remains mostly unexplored due to the added difficulty of reasoning over obstacles which greatly increases the computational time to generate high-quality collision-free grasps. In this work we address such limitations by introducing DDGC, a fast generative multi-finger grasp sampling method that can generate high quality grasps in cluttered scenes from a single RGB-D image. DDGC is built as a network that encodes scene information to produce coarse-to-fine collision-free grasp poses and configurations. We experimentally benchmark DDGC against the simulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and 7 real world scenes. The results show that DDGC outperforms the baseline on synthesizing high-quality grasps and removing clutter while being 5 times faster. This, in turn, opens up the door for using multi-finger grasps in practical applications which has so far been limited due to the excessive computation time needed by other methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202210_verdoja_generating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating people flow from architecture of real unseen
                   environments}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://iros2022-pnarude.github.io}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kucner, Tomasz Piotr and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Perception and Navigation for Autonomous
                   Robotics in Unstructured and Dynamic Environments (PNARUDE)''
                   workshop at the IEEE/RSJ Int.\ Conf.\ on Intelligent Robots and
                   Systems (IROS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Francesco Verdoja. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>