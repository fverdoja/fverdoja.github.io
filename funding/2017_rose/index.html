<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ROSE | francesco verdoja </title> <meta name="author" content="Francesco Verdoja"> <meta name="description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta name="keywords" content="robotics, mapping, computer vision, artificial intelligence, ai, machine learning, research, academia"> <meta property="og:site_name" content="francesco verdoja"> <meta property="og:type" content="website"> <meta property="og:title" content="francesco verdoja | ROSE"> <meta property="og:url" content="https://fverdoja.github.io/funding/2017_rose/"> <meta property="og:description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta property="og:image" content="assets/img/fra.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="ROSE"> <meta name="twitter:description" content="Academy Research Fellow, Intelligent Robotics, Aalto University, Finland "> <meta name="twitter:image" content="assets/img/fra.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Francesco Verdoja"
        },
        "url": "https://fverdoja.github.io/funding/2017_rose/",
        "@type": "WebSite",
        "description": "Academy Research Fellow, Intelligent Robotics, Aalto University, Finland
",
        "headline": "ROSE",
        
        "sameAs": ["https://orcid.org/0000-0002-9551-6186", "https://scholar.google.com/citations?user=3DDM3_kAAAAJ", "https://www.scopus.com/authid/detail.uri?authorId=56685641600", "https://github.com/fverdoja", "https://www.linkedin.com/in/fverdoja", "https://research.aalto.fi/en/persons/francesco-verdoja"],
        
        "name": "Francesco Verdoja",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fverdoja.github.io/funding/2017_rose/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">francesco</span> verdoja </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/events/">events </a> </li> <li class="nav-item active"> <a class="nav-link" href="/funding/">funding <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"><a class="nav-link" href="/assets/pdf/verdoja_francesco.pdf">cv</a></li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">ROSE</span> | Robots and the Future of Welfare Services </h1> <p class="post-description"></p> <ul class="post-tags fa-ul"> <li> <span class="fa-li"><i class="fa-solid fa-landmark fa-sm"></i></span><strong>Funded by:</strong> <a href="https://www.aka.fi/en/" rel="external nofollow noopener" target="_blank">Academy of Finland</a> </li> <li> <span class="fa-li"><i class="fa-solid fa-users fa-sm"></i></span><strong>Partners:</strong> Aalto University (PI: Prof. Ville Kyrki), LUT-University, Tampere University of Technology, University of Tampere, VTT, Laurea </li> <li> <span class="fa-li"><i class="fa-solid fa-user fa-sm"></i></span><strong>Role:</strong> Postdoctoral researcher, involvement in research supervision (from Sep 2017) </li> <li> <span class="fa-li"><i class="fa-solid fa-calendar fa-sm"></i></span><strong>Period:</strong> May 2015–Apr 2021 </li> <li> <span class="fa-li"><i class="fa-solid fa-link fa-sm"></i></span><a href="https://roseproject.aalto.fi/en/" rel="external nofollow noopener" target="_blank">Project website</a> </li> </ul> </header> <article> <h2 id="abstract">abstract</h2> <p>In post-industrial societies such as Finland the demand for welfare and health services is growing strongly and health and welfare services cover the majority of public expenditure. Service robots are believed to have great potential for the area, increasing productivity and enabling quality improvements and new business through novel services. The application area is nevertheless challenging due to the centrality of ethical, legal and social issues.</p> <p>ROSE project is a multidisciplinary study how advances in service robotics allow product and service innovation and renewal of welfare services, when such services are developed ethically and jointly with stakeholders. The development is studied on individual, institutional and societal levels, taking into account user needs, ethical issues, technological maturity, and the health care service system.</p> </article> <h2 id="related-publications">related publications</h2> <div class="publications"> <h2 class="bibliography">journal articles</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_2_21-480.webp 480w,/assets/img/publication_preview/ral_2_21-800.webp 800w,/assets/img/publication_preview/ral_2_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ral_2_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ral_2_21.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202110_lundell_ddgc" class="col-sm-8"> <div class="title">DDGC: Generative Deep Dexterous Grasping in Clutter</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.04783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483683" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=bKjBVwDBYvg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/aalto-intelligent-robotics/DDGC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Recent advances in multi-fingered robotic grasping have enabled fast 6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in cluttered scenes, on the other hand, remains mostly unexplored due to the added difficulty of reasoning over obstacles which greatly increases the computational time to generate high-quality collision-free grasps. In this work we address such limitations by introducing DDGC, a fast generative multi-finger grasp sampling method that can generate high quality grasps in cluttered scenes from a single RGB-D image. DDGC is built as a network that encodes scene information to produce coarse-to-fine collision-free grasp poses and configurations. We experimentally benchmark DDGC against the simulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and 7 real world scenes. The results show that DDGC outperforms the baseline on synthesizing high-quality grasps and removing clutter while being 5 times faster. This, in turn, opens up the door for using multi-finger grasps in practical applications which has so far been limited due to the excessive computation time needed by other methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202110_lundell_ddgc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{DDGC}: {Generative} {Deep} {Dexterous} {Grasping} in 
                   {Clutter}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{DDGC}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2021.3096239}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6899--6906}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RA-L</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_1_21-480.webp 480w,/assets/img/publication_preview/ral_1_21-800.webp 800w,/assets/img/publication_preview/ral_1_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ral_1_21.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ral_1_21.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202104_nguyen_le_probabilistic" class="col-sm-8"> <div class="title">Probabilistic Surface Friction Estimation Based on Visual and Haptic Measurements</div> <div class="author"> Tran Nguyen Le, <em>Francesco Verdoja</em>, Fares J. Abu-Dakka, and Ville Kyrki </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2010.08277" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=OXXGk5Byu3g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Accurately modeling local surface properties of objects is crucial to many robotic applications, from grasping to material recognition. Surface properties like friction are however difficult to estimate, as visual observation of the object does not convey enough information over these properties. In contrast, haptic exploration is time consuming as it only provides information relevant to the explored parts of the object. In this work, we propose a joint visuo-haptic object model that enables the estimation of surface friction coefficient over an entire object by exploiting the correlation of visual and haptic information, together with a limited haptic exploration by a robotic arm. We demonstrate the validity of the proposed method by showing its ability to estimate varying friction coefficients on a range of real multi-material objects. Furthermore, we illustrate how the estimated friction coefficients can improve grasping success rate by guiding a grasp planner toward high friction areas.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">202104_nguyen_le_probabilistic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic {Surface} {Friction} {Estimation} {Based} on 
                   {Visual} and {Haptic} {Measurements}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LRA.2021.3062585}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nguyen Le, Tran and Verdoja, Francesco and Abu-Dakka, Fares J. and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2838--2845}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">conference articles</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECMR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecmr_21-480.webp 480w,/assets/img/publication_preview/ecmr_21-800.webp 800w,/assets/img/publication_preview/ecmr_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ecmr_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecmr_21.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202108_dengler_online" class="col-sm-8"> <div class="title">Online Object-Oriented Semantic Mapping and Map Updating</div> <div class="author"> Nils Dengler, Tobias Zaenker, <em>Francesco Verdoja</em>, and Maren Bennewitz </div> <div class="periodical"> <em>In 2021 Eur. Conf. on Mobile Robots (ECMR)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2011.06895" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2011.06895" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/NilsDengler/sem_mapping" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Creating and maintaining an accurate representation of the environment is an essential capability for every service robot. Especially for household robots acting in indoor environments, semantic information is important. In this paper, we present a semantic mapping framework with modular map representations. Our system is capable of online mapping and object updating given object detections from RGB-D data and provides various 2D and 3D representations of the mapped objects. To undo wrong data associations, we perform a refinement step when updating object shapes. Furthermore, we maintain an existence likelihood for each object to deal with false positive and false negative detections and keep the map updated. Our mapping system is highly efficient and achieves a run time of more than 10 Hz. We evaluated our approach in various environments using two different robots, i.e., a Toyota HSR and a Fraunhofer Care-O-Bot-4. As the experimental results demonstrate, our system is able to generate maps that are close to the ground truth and outperforms an existing approach in terms of intersection over union, different distance metrics, and the number of correct object mappings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202108_dengler_online</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Online {Object}-{Oriented} {Semantic} {Mapping} and {Map} 
                   {Updating}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ECMR50962.2021.9568817}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 {Eur.} {Conf.} on {Mobile} {Robots} ({ECMR})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dengler, Nils and Zaenker, Tobias and Verdoja, Francesco and Bennewitz, Maren}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_21-480.webp 480w,/assets/img/publication_preview/icra_21-800.webp 800w,/assets/img/publication_preview/icra_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_21.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202105_lundell_multi-fingan" class="col-sm-8"> <div class="title">Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps</div> <div class="author"> Jens Lundell, Enric Corona, Tran Nguyen Le, <em>Francesco Verdoja</em>, Philippe Weinzaepfel, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Grégory Rogez, Francesc Moreno-Noguer, Ville Kyrki' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2021 IEEE Int. Conf. on Robotics and Automation (ICRA)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2012.09696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2012.09696" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/Multi-FinGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While there exists many methods for manipulating rigid objects with parallel-jaw grippers, grasping with multi-finger robotic hands remains a quite unexplored research topic. Reasoning and planning collision-free trajectories on the additional degrees of freedom of several fingers represents an important challenge that, so far, involves computationally costly and slow processes. In this work, we present Multi-FinGAN, a fast generative multi-finger grasp sampling method that synthesizes high quality grasps directly from RGB-D images in about a second. We achieve this by training in an end-to-end fashion a coarse-to-fine model composed of a classification network that distinguishes grasp types according to a specific taxonomy and a refinement network that produces refined grasp poses and joint angles. We experimentally validate and benchmark our method against a standard grasp-sampling method on 790 grasps in simulation and 20 grasps on a real Franka Emika Panda. All experimental results using our method show consistent improvements both in terms of grasp quality metrics and grasp success rate. Remarkably, our approach is up to 20-30 times faster than the baseline, a significant improvement that opens the door to feedback-based grasp re-planning and task informative grasping. Code is available at this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202105_lundell_multi-fingan</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-{FinGAN}: {Generative} {Coarse}-{To}-{Fine} {Sampling} of 
                   {Multi}-{Finger} {Grasps}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Multi-{FinGAN}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48506.2021.9561228}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                   ({ICRA})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Corona, Enric and Nguyen Le, Tran and Verdoja, Francesco and Weinzaepfel, Philippe and Rogez, Grégory and Moreno-Noguer, Francesc and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4495--4501}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MFI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mfi_20-480.webp 480w,/assets/img/publication_preview/mfi_20-800.webp 800w,/assets/img/publication_preview/mfi_20-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mfi_20.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mfi_20.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202009_zaenker_hypermap" class="col-sm-8"> <div class="title">Hypermap Mapping Framework and its Application to Autonomous Semantic Exploration</div> <div class="author"> Tobias Zaenker, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2020 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI)</em>, Sep 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.09526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1909.09526" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=mdxpBlK4qjk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Eruvae/hypermap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Modern intelligent and autonomous robotic applications often require robots to have more information about their environment than that provided by traditional occupancy grid maps. For example, a robot tasked to perform autonomous semantic exploration has to label objects in the environment it is traversing while autonomously navigating. To solve this task the robot needs to at least maintain an occupancy map of the environment for navigation, an exploration map keeping track of which areas have already been visited, and a semantic map where locations and labels of objects in the environment are recorded. As the number of maps required grows, an application has to know and handle different map representations, which can be a burden.We present the Hypermap framework, which can manage multiple maps of different types. In this work, we explore the capabilities of the framework to handle occupancy grid layers and semantic polygonal layers, but the framework can be extended with new layer types in the future. Additionally, we present an algorithm to automatically generate semantic layers from RGB-D images. We demonstrate the utility of the framework using the example of autonomous exploration for semantic mapping.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202009_zaenker_hypermap</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hypermap {Mapping} {Framework} and its {Application} to 
                   {Autonomous} {Semantic} {Exploration}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/MFI49285.2020.9235231}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 {IEEE} {Int.} {Conf.} on {Multisensor} {Fusion} and 
                   {Integration} for {Intelligent} {Systems} ({MFI})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zaenker, Tobias and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{133--139}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_20-480.webp 480w,/assets/img/publication_preview/icra_20-800.webp 800w,/assets/img/publication_preview/icra_20-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_20.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_20.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202005_lundell_beyond" class="col-sm-8"> <div class="title">Beyond Top-Grasps Through Scene Completion</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2020 IEEE Int. Conf. on Robotics and Automation (ICRA)</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.12908" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1909.12908" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=Pret0HNU5us" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">202005_lundell_beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond {Top}-{Grasps} {Through} {Scene} {Completion}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA40945.2020.9197320}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 {IEEE} {Int.} {Conf.} on {Robotics} and {Automation} 
                   ({ICRA})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{545--551}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_19-480.webp 480w,/assets/img/publication_preview/iros_19-800.webp 800w,/assets/img/publication_preview/iros_19-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/iros_19.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_19.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="201911_lundell_robust" class="col-sm-8"> <div class="title">Robust Grasp Planning Over Uncertain Shape Completions</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2019 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Nov 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1903.00645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1903.00645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/robust_grasp_planning_over_uncertain_shape_completions" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a method for planning robust grasps over uncertain shape completed objects. For shape completion, a deep neural network is trained to take a partial view of the object as input and outputs the completed shape as a voxel grid. The key part of the network is dropout layers which are enabled not only during training but also at run-time to generate a set of shape samples representing the shape uncertainty through Monte Carlo sampling. Given the set of shape completed objects, we generate grasp candidates on the mean object shape but evaluate them based on their joint performance in terms of analytical grasp metrics on all the shape candidates. We experimentally validate and benchmark our method against another state-of-the-art method with a Barrett hand on 90000 grasps in simulation and 200 grasps on a real Franka Emika Panda. All experimental results show statistically significant improvements both in terms of grasp quality metrics and grasp success rate, demonstrating that planning shape-uncertainty-aware grasps brings significant advantages over solely planning on a single shape estimate, especially when dealing with complex or unknown objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201911_lundell_robust</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Macau, China}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} 
                   {Completions}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS40897.2019.8967816}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                   {Intelligent} {Robots} and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1526--1532}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Humanoids</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/humanoids_19-480.webp 480w,/assets/img/publication_preview/humanoids_19-800.webp 800w,/assets/img/publication_preview/humanoids_19-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/humanoids_19.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="humanoids_19.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="201910_verdoja_deep" class="col-sm-8"> <div class="title">Deep Network Uncertainty Maps for Indoor Navigation</div> <div class="author"> <em>Francesco Verdoja</em>, Jens Lundell, and Ville Kyrki </div> <div class="periodical"> <em>In 2019 IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids)</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1809.04891v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1809.04891v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=J5GOfbPTH88" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/aalto-intelligent-robotics/uncertain_turtlebot_navigation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Most mobile robots for indoor use rely on 2D laser scanners for localization, mapping and navigation. These sensors, however, cannot detect transparent surfaces or measure the full occupancy of complex objects such as tables. Deep Neural Networks have recently been proposed to overcome this limitation by learning to estimate object occupancy. These estimates are nevertheless subject to uncertainty, making the evaluation of their confidence an important issue for these measures to be useful for autonomous navigation and mapping. In this work we approach the problem from two sides. First we discuss uncertainty estimation in deep models, proposing a solution based on a fully convolutional neural network. The proposed architecture is not restricted by the assumption that the uncertainty follows a Gaussian model, as in the case of many popular solutions for deep model uncertainty estimation, such as Monte-Carlo Dropout. We present results showing that uncertainty over obstacle distances is actually better modeled with a Laplace distribution. Then, we propose a novel approach to build maps based on Deep Neural Network uncertainty models. In particular, we present an algorithm to build a map that includes information over obstacle distance estimates while taking into account the level of uncertainty in each estimate. We show how the constructed map can be used to increase global navigation safety by planning trajectories which avoid areas of high uncertainty, enabling higher autonomy for mobile robots in indoor settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201910_verdoja_deep</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep {Network} {Uncertainty} {Maps} for {Indoor} {Navigation}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/Humanoids43949.2019.9035016}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 {IEEE-RAS} {Int.}\ {Conf.}\ on {Humanoid} {Robots} 
                   ({Humanoids})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Lundell, Jens and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112--119}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IROS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iros_18-480.webp 480w,/assets/img/publication_preview/iros_18-800.webp 800w,/assets/img/publication_preview/iros_18-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/iros_18.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iros_18.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="201810_lundell_hallucinating" class="col-sm-8"> <div class="title">Hallucinating robots: Inferring obstacle distances from partial laser measurements</div> <div class="author"> Jens Lundell, <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> <em>In 2018 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1805.12338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1805.12338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=J5GOfbPTH88" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/jsll/IROS2018-Hallucinating-Robots" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Many mobile robots rely on 2D laser scanners for localization, mapping, and navigation. However, those sensors are unable to correctly provide distance to obstacles such as glass panels and tables whose actual occupancy is invisible at the height the sensor is measuring. In this work, instead of estimating the distance to obstacles from richer sensor readings such as 3D lasers or RGBD sensors, we present a method to estimate the distance directly from raw 2D laser data. To learn a mapping from raw 2D laser distances to obstacle distances we frame the problem as a learning task and train a neural network formed as an autoencoder. A novel configuration of network hyperparameters is proposed for the task at hand and is quantitatively validated on a test set. Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the trained network can successfully infer obstacle distances from partial 2D laser readings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">201810_lundell_hallucinating</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Madrid, Spain}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hallucinating robots: {Inferring} obstacle distances from
                   partial laser measurements}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS.2018.8594399}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2018 {IEEE}/{RSJ} {Int.}\ {Conf.}\ on 
                   {Intelligent} {Robots} and {Systems} ({IROS})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lundell, Jens and Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4781--4787}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">workshop articles</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icml_21-480.webp 480w,/assets/img/publication_preview/icml_21-800.webp 800w,/assets/img/publication_preview/icml_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icml_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml_21.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202107_verdoja_notes" class="col-sm-8"> <div class="title">Notes on the Behavior of MC Dropout</div> <div class="author"> <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> Jul 2021 </div> <div class="periodical"> Presented at the “Uncertainty and Robustness in Deep Learning (UDL)” workshop at the 2021 Int. Conf. on Machine Learning (ICML) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2008.02627" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2008.02627" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aalto-intelligent-robotics/mc-dropout-notebooks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Among the various options to estimate uncertainty in deep neural networks, Monte-Carlo dropout is widely popular for its simplicity and effectiveness. However the quality of the uncertainty estimated through this method varies and choices in architecture design and in training procedures have to be carefully considered and tested to obtain satisfactory results. In this paper we present a study offering a different point of view on the behavior of Monte-Carlo dropout, which enables us to observe a few interesting properties of the technique to keep in mind when considering its use for uncertainty estimation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202107_verdoja_notes</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Notes on the {Behavior} of {MC} {Dropout}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://sites.google.com/view/udlworkshop2021}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Uncertainty and Robustness in Deep
                   Learning (UDL)'' workshop at the 2021 Int.\ Conf.\ on Machine 
                   Learning (ICML)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/icra_w_20-480.webp 480w,/assets/img/publication_preview/icra_w_20-800.webp 800w,/assets/img/publication_preview/icra_w_20-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/icra_w_20.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icra_w_20.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202005_verdoja_potential" class="col-sm-8"> <div class="title">On the Potential of Smarter Multi-layer Maps</div> <div class="author"> <em>Francesco Verdoja</em>, and Ville Kyrki </div> <div class="periodical"> May 2020 </div> <div class="periodical"> Presented at the “Perception, Action, Learning (PAL)” workshop at the 2020 IEEE Int. Conf. on Robotics and Automation (ICRA) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.11094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/pdf/2005.11094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=s6LWzBsc7Ao" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>The most common way for robots to handle environmental information is by using maps. At present, each kind of data is hosted on a separate map, which complicates planning because a robot attempting to perform a task needs to access and process information from many different maps. Also, most often correlation among the information contained in maps obtained from different sources is not evaluated or exploited. In this paper, we argue that in robotics a shift from single-source maps to a multi-layer mapping formalism has the potential to revolutionize the way robots interact with knowledge about their environment. This observation stems from the raise in metric-semantic mapping research, but expands to include in its formulation also layers containing other information sources, e.g., people flow, room semantic, or environment topology. Such multi-layer maps, here named hypermaps, not only can ease processing spatial data information but they can bring added benefits arising from the interaction between maps. We imagine that a new research direction grounded in such multi-layer mapping formalism for robots can use artificial intelligence to process the information it stores to present to the robot task-specific information simplifying planning and bringing us one step closer to high-level reasoning in robots.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@online</span><span class="p">{</span><span class="nl">202005_verdoja_potential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the {Potential} of {Smarter} {Multi}-layer {Maps}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://mit-spark.github.io/PAL-ICRA2020}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Verdoja, Francesco and Kyrki, Ville}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Presented at the ``Perception, Action, Learning (PAL)'' 
                   workshop at the 2020 IEEE Int.\ Conf.\ on Robotics and 
                   Automation (ICRA)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">technical reports</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rose_21-480.webp 480w,/assets/img/publication_preview/rose_21-800.webp 800w,/assets/img/publication_preview/rose_21-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/rose_21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rose_21.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="202100_kyrki_robots" class="col-sm-8"> <div class="title">Robots and the Future of Welfare Services: A Finnish Roadmap</div> <div class="author"> Ville Kyrki, Iina Aaltonen, Antti Ainasoja, Päivi Heikkilä, Sari Heikkinen, and <span class="more-authors" title="click to view 31 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '31 more authors' ? 'Lea Hennala, Pertti Koistinen, Joni Kämäräinen, Kalle Laakso, Arto Laitinen, Hanna Lammi, Marinka Lanne, Inka Lappalainen, Hannu Lehtinen, Paula Lehto, Teppo Leppälahti, Jens Lundell, Helinä Melkas, Marketta Niemelä, Satu Parjanen, Jaana Parviainen, Satu Pekkarinen, Jari Pirhonen, Jaakko Porokuokka, Teemu Rantanen, Ismo Ruohomäki, Riika Saurio, Otto Sahlgren, Tuomo Särkikoski, Heli Talja, Antti Tammela, Outi Tuisku, Tuuli Turja, Lina Van Aerschot, Francesco Verdoja, Kari Välimäki' : '31 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '6'); ">31 more authors</span> </div> <div class="periodical"> May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aaltodoc.aalto.fi/bitstream/123456789/107147/1/isbn9789526403236.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This roadmap summarises a six-year multidisciplinary research project called Robots and the Future of Welfare Services (ROSE), funded by the Strategic Research Council (SRC) established within the Academy of Finland. The objective of the project was to study the current and expected technical opportunities and applications of robotics in welfare services, particularly in care services for older people. The research was carried out at three levels: individual, organisational and societal. The roadmap provides highlights of the various research activities of ROSE. We have studied the perspectives of older adults and care professionals as users of robots, how care organisations are able to adopt and utilise robots in their services, how technology companies find robots as business opportunity, and how the care robotics innovation ecosystem is evolving. Based on these and other studies, we evaluate the development and use of robots in care for older adults in terms of social, ethical-philosophical and political impacts as well as the public discussion on care robots. It appears that there are many single- or limited-purpose robot applications already commercially available in care services for older adults. To be widely adopted, robots should still increase maturity to be able to meet the requirements of care environments, such as in terms of their ability to move in smaller crowded spaces, easy and natural user interaction, and task flexibility. The roadmap provides visions of what could be technically expected in five and ten years. However, at the same time, organisations’ capabilities of adopting new technology and integrating it into services should be supported for them to be able to realise the potential of robots for the benefits of care workers and older persons, as well as the whole society. This roadmap also provides insight into the wider impacts and risks of robotization in society and how to steer it in a responsible way, presented as eight policy recommendations. We also discuss the ROSE project research as a multidisciplinary activity and present lessons learnt.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@techreport</span><span class="p">{</span><span class="nl">202100_kyrki_robots</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robots and the {Future} of {Welfare} {Services}: {A} {Finnish} 
                   {Roadmap}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Robots and the {Future} of {Welfare} {Services}}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">institution</span> <span class="p">=</span> <span class="s">{Aalto University}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kyrki, Ville and Aaltonen, Iina and Ainasoja, Antti and Heikkilä, Päivi and Heikkinen, Sari and Hennala, Lea and Koistinen, Pertti and Kämäräinen, Joni and Laakso, Kalle and Laitinen, Arto and Lammi, Hanna and Lanne, Marinka and Lappalainen, Inka and Lehtinen, Hannu and Lehto, Paula and Leppälahti, Teppo and Lundell, Jens and Melkas, Helinä and Niemelä, Marketta and Parjanen, Satu and Parviainen, Jaana and Pekkarinen, Satu and Pirhonen, Jari and Porokuokka, Jaakko and Rantanen, Teemu and Ruohomäki, Ismo and Saurio, Riika and Sahlgren, Otto and Särkikoski, Tuomo and Talja, Heli and Tammela, Antti and Tuisku, Outi and Turja, Tuuli and Aerschot, Lina Van and Verdoja, Francesco and Välimäki, Kari}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{72}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Francesco Verdoja. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: October 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>